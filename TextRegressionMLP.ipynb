{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sayantika-Nag/ML-DataAnalytics/blob/main/TextRegressionMLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsywQWEI8pzj"
      },
      "source": [
        "# Overview\n",
        "In this assignment you will implement a simple linear neural network that reads in text and uses pretrained embeddings to predict the **happiness intensity** of the text.\n",
        "You'll fit the network weights using the analytic expression for linear regression with L2 regularization we discussed in class.\n",
        "\n",
        "For this assignment we will use the functionality of PyTorch, HuggingFace \"transformers\" library for getting pretrained models, \"pandas\" for data loading, matplotlib for visualization. Before you start, make sure you have installed all those packages in your local Jupyter instance. Or use Google Colab (which has everything you need pre-installed).\n",
        "\n",
        "Read **all** cells carefully and answer **all** parts (both text and missing code). You will complete all the code marked `TODO` and print desired results.\n",
        "\n",
        "### Exporting the Notebook to PDF\n",
        "To generate a nice looking PDF of your completed notebook, either use \"Print as PDF\" from Google colab (using Chrome), or if you are running locally, run the following command in the last cell:\n",
        "```python\n",
        "!jupyter nbconvert --to pdf --output=yourname_submission.pdf hw1.ipynb\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NM7-Cx4H-jTt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "#%matplotlib inline\n",
        "\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "#display(HTML(\"\"\"\n",
        "#<style>\n",
        "#div.output pre {\n",
        "    #white-space: pre-wrap;      /* Wrap long lines */\n",
        "    #word-break: break-word;     /* Break words if necessary */\n",
        "#}\n",
        "#</style>\n",
        "#\"\"\"))  # prevent long lines from making unreadable PDF subs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VR37r18FtEPo"
      },
      "source": [
        "# **Getting and processing data**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2-zIA5vYOHA"
      },
      "outputs": [],
      "source": [
        "# Load dataset and visualize\n",
        "train_file = 'EI-reg-En-joy-train.txt'\n",
        "val_file = '2018-EI-reg-En-joy-dev.txt'\n",
        "df_train = pd.read_csv(train_file, sep='\\t')\n",
        "df_val = pd.read_csv(val_file, sep='\\t')\n",
        "\n",
        "tweets_train = df_train['Tweet'].tolist()  # Create a list of tweets\n",
        "tweets_val = df_val['Tweet'].tolist()\n",
        "\n",
        "# Create a list of intensity scores\n",
        "y_train = torch.tensor(df_train['Intensity Score'], dtype=torch.float32)  # match to dtype of embedding\n",
        "y_val = torch.tensor(df_val['Intensity Score'], dtype=torch.float32)\n",
        "\n",
        "print('Score - Tweet')\n",
        "for i in range(5):\n",
        "    print('{:0.2f} - {}'.format(y_train[i], tweets_train[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tk-40Z_FaReQ"
      },
      "outputs": [],
      "source": [
        "print(len(tweets_train)); print(y_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding Model :\n",
        "load a pretrained model and write a function that embeds sentences into vector space"
      ],
      "metadata": {
        "id": "Tamqx6LR-KIl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9E5yknPYT6OL"
      },
      "outputs": [],
      "source": [
        "# 1. Embedding model :\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"  # Many possibilities on huggingface.com\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "embedding_model = AutoModel.from_pretrained(model_name)\n",
        "def embed_sentence(embedding_model, tokenizer, sentence):\n",
        "    \"\"\"Function to embed a sentence as a vector using a pre-trained model.\"\"\"\n",
        "    input_id_tensor=tokenizer(sentence,return_tensors='pt')['input_ids']\n",
        "    with torch.no_grad():\n",
        "        output=embedding_model(input_id_tensor).last_hidden_state\n",
        "    mean_embeddings=output.mean(dim=1)\n",
        "    return mean_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NwgI33xaReQ"
      },
      "outputs": [],
      "source": [
        "## Check the embedding function:\n",
        "s=tweets_train[0]\n",
        "embed_sentence(embedding_model,tokenizer,s)\n",
        "\n",
        "s1=tweets_train[:10]\n",
        "tensor_list=[]\n",
        "for s in s1:\n",
        "    es=embed_sentence(embedding_model,tokenizer,s)\n",
        "    print(es.shape)\n",
        "    tensor_list.append(es)\n",
        "stacked_embeddings=torch.vstack(tensor_list)\n",
        "print(stacked_embeddings.shape)  # should be (10, embedding_dim)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roqBNu3FT6OL"
      },
      "outputs": [],
      "source": [
        "# 2.Use embedding model to turn text into a matrix of embeddings.\n",
        "# Create a pytorch matrix where each row corresponds to a tweet,\n",
        "# and the number of columns/features is the size of the embedding\n",
        "\n",
        "X_train =  torch.vstack([embed_sentence(embedding_model,tokenizer,s) for s in tweets_train])\n",
        "X_val =  torch.vstack([embed_sentence(embedding_model,tokenizer,s) for s in tweets_val])\n",
        "\n",
        "print(X_train.shape, X_val.shape)  # torch.Size([1616, 384]) torch.Size([290, 384])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRxPXQ1KT6OL"
      },
      "source": [
        "# Define the Regression model :\n",
        "The model implements a linear model $$\\hat{y}(x) = \\mathbf w \\cdot \\mathbf x + b.$$ The \"fit\" method should use the analytic formula discussed in class to minimize the loss, $$L(\\mathbf w, b) = \\sum_{i=1}^N (y_i -  \\hat y(\\mathbf x_i))^2 + \\gamma (\\sum_{j=1}^d w_i^2 + b^2)$$\n",
        "Our L2 regularizer is applied to all the parameters for simplicity. But note that it is common to only apply L2 regularization to the weights and not the biases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKsbW-I7aReR"
      },
      "outputs": [],
      "source": [
        "## check codes:\n",
        "X = X_train ; y = y_train\n",
        "ones = torch.ones(X.shape[0], 1)\n",
        "X_aug = torch.cat([X, ones], dim=1)\n",
        "print(X_aug.shape)  # should be (1616, embedding_dim + 1)\n",
        "\n",
        "gamma = 0.0  # regularization strength\n",
        "#Analytic solution: (X^T X + gamma*I)^-1 * X^T * y\n",
        "d_aug = X_aug.shape[1]\n",
        "print(d_aug)  # should be embedding_dim + 1\n",
        "I = torch.eye(d_aug)\n",
        "# Solving: w = inv(X_aug.T @ X_aug + gamma * I) @ X_aug.T @ y\n",
        "XTX_reg = X_aug.t() @ X_aug + gamma * I\n",
        "print(XTX_reg.shape)\n",
        "XTX_reg_inv = torch.linalg.inv(XTX_reg)\n",
        "print(XTX_reg_inv.shape)\n",
        "w= XTX_reg_inv @ X_aug.t() @ y\n",
        "print(w.shape)  # should be (embedding_dim + 1, )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "265qk4h2YUrc"
      },
      "outputs": [],
      "source": [
        "class MyLinearNet(torch.nn.Module):\n",
        "    def __init__(self, input_embedding_size):\n",
        "        super().__init__()  # init superclass - enables many pytorch model capabilities\n",
        "        self.d = input_embedding_size  # Often convenient to store this (not a \"Parameter\" though as we don't train it)\n",
        "        # TODO [1 point]: define weights and bias with \"Parameters\"\n",
        "        self.w = torch.nn.Parameter(torch.randn(self.d))  # weights\n",
        "        self.b = torch.nn.Parameter(torch.randn(()))  # bias\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Implement a linear model\"\"\"\n",
        "        # TODO [1 point]: implement linear model, in terms of weights and biases\n",
        "        # It should work on a single x, or a batch\n",
        "        y_hat = x @ self.w + self.b  # shape (batch_size,)\n",
        "        return y_hat\n",
        "\n",
        "    def fit(self, X, y, gamma=0.):\n",
        "        \"\"\"Given a data matrix, X, and a vector of labels for each row, y,\n",
        "        analytically fit the parameters of the linear model.\n",
        "        Use an L2 regularizer with strength 'gamma'.\"\"\"\n",
        "        # TODO [3 points]: Use linear regression formula to set weight and bias parameters\n",
        "\n",
        "        # (a) First, construct the augmented data matrix as discussed in class\n",
        "        ones = torch.ones(X.shape[0], 1)\n",
        "        X_aug = torch.cat([X, ones], dim=1)  # shape (n, d+1)\n",
        "        # (b) Next, use matrix multiplication and torch.linalg.inv to implement the analytic solution\n",
        "        #Analytic solution: (X^T X + gamma*I)^-1 * X^T * y\n",
        "        d_aug = X_aug.shape[1]\n",
        "        I = torch.eye(d_aug)\n",
        "        # Solving: w = inv(X_aug.T @ X_aug + gamma * I) @ X_aug.T @ y\n",
        "        A = X_aug.t() @ X_aug + gamma * I\n",
        "        B= X_aug.t() @ y\n",
        "        w = torch.linalg.solve(A, B)  # shape (d_aug,)\n",
        "        # Note the size should be d+1, as the bias is included\n",
        "\n",
        "        # (c) Put the solution (which includes weights and biases) into parameters\n",
        "        # Use \"data\" to update parameters without affecting computation graph\n",
        "        # (Kind of a subtle point - no need to modify my code below)\n",
        "        self.w.data = w[:self.d]\n",
        "        self.b.data = w[-1]\n",
        "\n",
        "\n",
        "\n",
        "# --- Gemini UNIT TEST FOR MyLinearNet ---\n",
        "# This cell checks if your model structure is correct before training. No need to modify.\n",
        "def test_my_linear_net():\n",
        "    print(\"Testing MyLinearNet...\")\n",
        "    d_test = 10\n",
        "    model_test = MyLinearNet(d_test)\n",
        "\n",
        "    # 1. Check Parameter Shapes\n",
        "    assert model_test.w.shape == (d_test,), f\"Weight shape incorrect. Expected ({d_test},), got {model_test.w.shape}\"\n",
        "    assert model_test.b.shape == () or model_test.b.shape == (1,), f\"Bias shape incorrect.\"\n",
        "    print(\"  [+] Parameter shapes look good.\")\n",
        "\n",
        "    # 2. Check Forward Pass dimensions\n",
        "    batch_size = 5\n",
        "    x_test = torch.randn(batch_size, d_test)\n",
        "    y_test = model_test(x_test)\n",
        "    assert y_test.shape == (batch_size,), f\"Output shape incorrect. Expected ({batch_size},), got {y_test.shape}\"\n",
        "    print(\"  [+] Forward pass output dimensions look good.\")\n",
        "\n",
        "    # 3. Check Fit (just that it runs without error on dummy data)\n",
        "    y_target = torch.randn(batch_size)\n",
        "    try:\n",
        "        model_test.fit(x_test, y_target)\n",
        "        print(\"  [+] Fit method ran without crashing.\")\n",
        "    except Exception as e:\n",
        "        print(f\"  [-] Fit method crashed: {e}\")\n",
        "        return\n",
        "\n",
        "    print(\"Unit test passed! âœ…\")\n",
        "\n",
        "test_my_linear_net()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQhiQjNjT6OM"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7Z-aB0AaReR"
      },
      "source": [
        "## 1. First look at training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXtUFymP9M0P"
      },
      "outputs": [],
      "source": [
        "def loss(model, X, y):\n",
        "    # TODO [1 point]: implement the mean square error loss\n",
        "    with torch.no_grad():\n",
        "        y_hat = model(X)\n",
        "    mse_loss = torch.mean((y_hat - y) ** 2)\n",
        "    return mse_loss\n",
        "\n",
        "d = X_train.shape[1]  # embedding dimension\n",
        "model = MyLinearNet(d)\n",
        "\n",
        "loss_train = loss(model, X_train, y_train)\n",
        "loss_val = loss(model, X_val, y_val)\n",
        "print(\"\\nLoss on train and validation BEFORE fitting.\\nTrain: {:0.3f}, Val: {:0.3f}\".format(loss_train, loss_val))\n",
        "\n",
        "model.fit(X_train, y_train,gamma=0.)  # fit without regularization\n",
        "loss_train = loss(model, X_train, y_train)\n",
        "loss_val = loss(model, X_val, y_val)\n",
        "print(\"\\nLoss on train and validation AFTER fitting WITHOUT regularization.\\nTrain: {:0.3f}, Val: {:0.3f}\".format(loss_train, loss_val))\n",
        "\n",
        "#model.fit(X_train, y_train, gamma=10.)  # because of how we scaled the loss, the regularizer values could be quite large\n",
        "#loss_train = loss(model, X_train, y_train)\n",
        "#loss_val = loss(model, X_val, y_val)\n",
        "# TODO [4 points]: Show that Train loss is reduced below 0.02\n",
        "# and Validation loss is reduced below 0.05, at least\n",
        "# Adjust the regularizer strength to improve validation loss.\n",
        "print(\"\\nLoss on train and validation AFTER fitting WITH regularization.\")\n",
        "\n",
        "\n",
        "# Check for best gamma value\n",
        "gammas = [0., 0.01, 0.1, 1., 10., 100., 200., 500.]\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "for gamma in gammas:\n",
        "    model.fit(X_train, y_train, gamma=gamma)\n",
        "    train_loss = loss(model, X_train, y_train)\n",
        "    val_loss = loss(model, X_val, y_val)\n",
        "    train_losses.append(train_loss.item())\n",
        "    val_losses.append(val_loss.item())\n",
        "\n",
        "print(\"\\nResults for different regularization strengths (gamma):\")\n",
        "print(\"\\nGamma\\tTrain Loss\\tVal Loss\")\n",
        "for g, tr_loss, v_loss in zip(gammas, train_losses, val_losses):\n",
        "    print(\"{:0.2f}\\t{:0.3f}\\t\\t{:0.3f}\".format(g, tr_loss, v_loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ja1KrbD8aReR"
      },
      "outputs": [],
      "source": [
        "final_model = MyLinearNet(X_train.shape[1])\n",
        "final_model.fit(X_train, y_train, gamma=1.0)  # best gamma from above\n",
        "print(\"\\nFinal model trained with gamma=1.0\")\n",
        "train_loss = loss(final_model, X_train, y_train)\n",
        "val_loss = loss(final_model, X_val, y_val)\n",
        "print(\"train loss: {:0.3f}, val loss: {:0.3f}\".format(train_loss, val_loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuK9bpcTaReS"
      },
      "source": [
        "## 2. Visualize correlation between predicted and true labels, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-r__jPyk-Em-"
      },
      "outputs": [],
      "source": [
        "# Use only the regularized model for the results below:\n",
        "\n",
        "# Create a scatter plot of the actual vs. predicted values of `y` using this function.\n",
        "def plot(y_train, y_hat_train, y_val, y_hat_val):\n",
        "    fig, ax = plt.subplots(1)\n",
        "    ax.scatter(y_train, y_hat_train, alpha=0.4, label='train')\n",
        "    ax.scatter(y_val, y_hat_val, label='val')\n",
        "    ax.set_xlabel('y - ground truth joy intensity')\n",
        "    ax.set_ylabel('Predicted y')\n",
        "    ax.legend()\n",
        "\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    y_hat_train = final_model(X_train)\n",
        "    y_hat_val = final_model(X_val)\n",
        "\n",
        "plot(y_train, y_hat_train, y_val, y_hat_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTSktPI5aReS"
      },
      "source": [
        "# Model deep dive :\n",
        "Construct five original sentences and output their happy scores to try to understand how the model works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUKr8Dx2-UxU"
      },
      "outputs": [],
      "source": [
        "# Put in a sample sentence of your own construction and output the \"joy meter\" for a happy and sad sentence\n",
        "sentences = [\n",
        "    \"I am sad\",\n",
        "    \"dead\",\n",
        "    \"happy\",\n",
        "    \"dgr efef3v sew3defew ssdeereve wecd eveverve cewfrg4rg\",\n",
        "    \"dgr efef3v sew3defew ssdeereve wecd eveverve cewfrg4rg :) :(\",\n",
        "    \"dgr efef3v sew3defew ssdeereve wecd eveverve cewfrg4rg happy\",\n",
        "    \"Hope the world becomes a better place\",\n",
        "    \"I enjoy learning new things\"\n",
        "]\n",
        "\n",
        "# Print happy score for each sentence\n",
        "for s in sentences:\n",
        "    x_s = embed_sentence(embedding_model,tokenizer,s)  # shape (1, embedding_dim)\n",
        "    with torch.no_grad():\n",
        "        y_hat_s = final_model(x_s)  # shape (1,)\n",
        "    print(f\"Sentence: '{s}' => Predicted Joy Intensity: {y_hat_s.item():0.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrXXaX07T6ON"
      },
      "source": [
        "# Compare embeddings:\n",
        "\n",
        "Tried different embedding model\n",
        "\n",
        "-    \"bert-base-uncased\". Famous and a relatively fast model, but not specifically tuned for similarity.\n",
        "- \"sentence-transformers/all-mpnet-base-v2\", a strong baseline for sentence embedding, but slower to run than BERT, MiniLM\n",
        "\n",
        "If you go any bigger, you'll probably need to run on GPUs.\n",
        "https://huggingface.co/spaces/mteb/leaderboard\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSvdafr5aReS"
      },
      "source": [
        "## 1.Bert Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3se61hxpaReS"
      },
      "outputs": [],
      "source": [
        "# Get the validation loss using a different text embedding\n",
        "\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "embedding_model = AutoModel.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMYjhguRaReS"
      },
      "outputs": [],
      "source": [
        "X_train =  torch.vstack([embed_sentence(embedding_model,tokenizer,s) for s in tweets_train])\n",
        "X_val =  torch.vstack([embed_sentence(embedding_model,tokenizer,s) for s in tweets_val])\n",
        "\n",
        "print(X_train.shape, X_val.shape)  # torch.Size([1616, 384]) torch.Size([290, 384])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnYvxuTMaReS"
      },
      "outputs": [],
      "source": [
        "bert_model = MyLinearNet(X_train.shape[1])\n",
        "bert_model.fit(X_train, y_train, gamma=0)\n",
        "train_loss = loss(bert_model, X_train, y_train)\n",
        "val_loss = loss(bert_model, X_val, y_val)\n",
        "print(\"\\nBERT Embedding Model - without regularization\" \\\n",
        "\"\\n Train loss: {:0.3f}, Val loss: {:0.3f}\".format(train_loss, val_loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2ly7RIcaReS"
      },
      "outputs": [],
      "source": [
        "# Check for best gamma value\n",
        "gammas = [0., 0.01, 0.1, 1., 10., 100., 200., 500.]\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "for gamma in gammas:\n",
        "    bert_model.fit(X_train, y_train, gamma=gamma)\n",
        "    train_loss = loss(bert_model, X_train, y_train)\n",
        "    val_loss = loss(bert_model, X_val, y_val)\n",
        "    train_losses.append(train_loss.item())\n",
        "    val_losses.append(val_loss.item())\n",
        "print(\"\\nBERT Embedding Model - with regularization\")\n",
        "print(\"\\nResults for different regularization strengths (gamma):\")\n",
        "print(\"\\nGamma\\tTrain Loss\\tVal Loss\")\n",
        "for g, tr_loss, v_loss in zip(gammas, train_losses, val_losses):\n",
        "    print(\"{:0.2f}\\t{:0.3f}\\t\\t{:0.3f}\".format(g, tr_loss, v_loss))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhTaK5dJaReS"
      },
      "outputs": [],
      "source": [
        "#Final model training with best gamma\n",
        "bert_model.fit(X_train, y_train, gamma=1.0)\n",
        "train_loss = loss(bert_model, X_train, y_train)\n",
        "val_loss = loss(bert_model, X_val, y_val)\n",
        "print(\"\\nFinal BERT model trained with gamma=1.0\")\n",
        "print(\"train loss: {:0.3f}, val loss: {:0.3f}\".format(train_loss, val_loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdGVPCYeaReS"
      },
      "outputs": [],
      "source": [
        "# Put in a sample sentence of your own construction and output the \"joy meter\" for a happy and sad sentence\n",
        "sentences = [\n",
        "    \"I am sad\",\n",
        "    \"dead\",\n",
        "    \"happy\",\n",
        "    \"dgr efef3v sew3defew ssdeereve wecd eveverve cewfrg4rg\",\n",
        "    \"dgr efef3v sew3defew ssdeereve wecd eveverve cewfrg4rg :) :(\",\n",
        "    \"dgr efef3v sew3defew ssdeereve wecd eveverve cewfrg4rg happy\",\n",
        "    \"Hope the world becomes a better place\",\n",
        "    \"I enjoy learning new things\"\n",
        "]\n",
        "\n",
        "#  Print happy score for each sentence\n",
        "for s in sentences:\n",
        "    x_s = embed_sentence(embedding_model,tokenizer,s)  # shape (1, embedding_dim)\n",
        "    with torch.no_grad():\n",
        "        y_hat_s = bert_model(x_s)  # shape (1,)\n",
        "    print(f\"Sentence: '{s}' => Predicted Joy Intensity: {y_hat_s.item():0.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDCWZIFTaReS"
      },
      "source": [
        "## 2. all-mpnet-base-v2 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cz8dvZ1aReS"
      },
      "outputs": [],
      "source": [
        "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "embedding_model = AutoModel.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpKqiIQ5aReT"
      },
      "outputs": [],
      "source": [
        "X_train =  torch.vstack([embed_sentence(embedding_model,tokenizer,s) for s in tweets_train])\n",
        "X_val =  torch.vstack([embed_sentence(embedding_model,tokenizer,s) for s in tweets_val])\n",
        "\n",
        "print(X_train.shape, X_val.shape)  # torch.Size([1616, 384]) torch.Size([290, 384])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mY1ni78paReT"
      },
      "outputs": [],
      "source": [
        "mpnet_model = MyLinearNet(X_train.shape[1])\n",
        "mpnet_model.fit(X_train, y_train, gamma=0)\n",
        "train_loss = loss(mpnet_model, X_train, y_train)\n",
        "val_loss = loss(mpnet_model, X_val, y_val)\n",
        "print(\"\\nMPNet Embedding Model - without regularization\" \\\n",
        "\"\\n Train loss: {:0.3f}, Val loss: {:0.3f}\".format(train_loss, val_loss))\n",
        "\n",
        "\n",
        "# Check for best gamma value\n",
        "gammas = [0., 0.01, 0.1, 1., 10., 100., 200., 500.]\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "for gamma in gammas:\n",
        "    mpnet_model.fit(X_train, y_train, gamma=gamma)\n",
        "    train_loss = loss(mpnet_model, X_train, y_train)\n",
        "    val_loss = loss(mpnet_model, X_val, y_val)\n",
        "    train_losses.append(train_loss.item())\n",
        "    val_losses.append(val_loss.item())\n",
        "print(\"\\nMPNet Embedding Model - with regularization\")\n",
        "print(\"\\nResults for different regularization strengths (gamma):\")\n",
        "print(\"\\nGamma\\tTrain Loss\\tVal Loss\")\n",
        "for g, tr_loss, v_loss in zip(gammas, train_losses, val_losses):\n",
        "    print(\"{:0.2f}\\t{:0.3f}\\t\\t{:0.3f}\".format(g, tr_loss, v_loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDp3i9aXaReT"
      },
      "outputs": [],
      "source": [
        "# final model training with best gamma\n",
        "mpnet_model.fit(X_train, y_train, gamma=1.0)\n",
        "print(\"\\nFinal MPNet model trained with gamma=1.0\")\n",
        "train_loss = loss(mpnet_model, X_train, y_train)\n",
        "val_loss = loss(mpnet_model, X_val, y_val)\n",
        "print(\"train loss: {:0.3f}, val loss: {:0.3f}\".format(train_loss, val_loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XmaMA3LaReT"
      },
      "outputs": [],
      "source": [
        "# Put in a sample sentence of your own construction and output the \"joy meter\" for a happy and sad sentence\n",
        "sentences = [\n",
        "    \"I am sad\",\n",
        "    \"dead\",\n",
        "    \"happy\",\n",
        "    \"dgr efef3v sew3defew ssdeereve wecd eveverve cewfrg4rg\",\n",
        "    \"dgr efef3v sew3defew ssdeereve wecd eveverve cewfrg4rg :) :(\",\n",
        "    \"dgr efef3v sew3defew ssdeereve wecd eveverve cewfrg4rg happy\",\n",
        "    \"Hope the world becomes a better place\",\n",
        "    \"I enjoy learning new things\"\n",
        "]\n",
        "\n",
        "# Print happy score for each sentence\n",
        "for s in sentences:\n",
        "    x_s = embed_sentence(embedding_model,tokenizer,s)  # shape (1, embedding_dim)\n",
        "    with torch.no_grad():\n",
        "        y_hat_s = mpnet_model(x_s)  # shape (1,)\n",
        "    print(f\"Sentence: '{s}' => Predicted Joy Intensity: {y_hat_s.item():0.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKJuxWcdaReT"
      },
      "source": [
        "# Conclusion:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atjJTWFIaReT"
      },
      "source": [
        "### Model 1 :\n",
        "* model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "* regularization with gamma = 1.0\n",
        "* train loss = 0.014    validation loss = 0.027\n",
        "* adding regularization decreased the validation loss by ~ 6.9%\n",
        "\n",
        "### Model 2:\n",
        "* model_name = \"bert-base-uncased\"\n",
        "* regularization with gamma = 1.0\n",
        "* train loss = 0.010    validation loss =  0.033\n",
        "* adding regularization decreased the validation loss by ~25%\n",
        "\n",
        "### Model 3:\n",
        "* model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "* regularization with gamma = 1.0\n",
        "* train loss = 0.009    validation loss = 0.024\n",
        "* adding regularization decreased the validation loss by ~38.46%\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "abIby-qOapVf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "data-sc (3.12.10)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}