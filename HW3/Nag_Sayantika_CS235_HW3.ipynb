{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "5Th-5nwu91sy"
      },
      "outputs": [],
      "source": [
        "# HW 3 - NYT Articles Notebook\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "import warnings\n",
        "\n",
        "# This suppresses all warnings (UserWarning, DeprecationWarning, etc.)\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OCEdoudKNaM",
        "outputId": "6b27adc2-68b8-45a5-eb49-659942b11860"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                text   label\n",
            "0  (reuters) - carlos tevez sealed his move to ju...  sports\n",
            "1  if professional pride and strong defiance can ...  sports\n",
            "2  palermo, sicily — roberta vinci beat top-seede...  sports\n",
            "3  spain's big two soccer teams face a pair of it...  sports\n",
            "4  the argentine soccer club san lorenzo complete...  sports\n",
            "(11519, 2)\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"nyt.csv\")\n",
        "print(df.head())\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**1. Data preparation**\n",
        "* Train-Validation-Test -> 80:10:10 split with random_state=42\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "distinct y-values--['sports' 'business' 'politics']\n",
            "Total rows 11519, 80%of it = 9215.2, 10%of it= 1151.9\n",
            "Train size: 9215\n",
            "Validation size: 1152\n",
            "Test size: 1152\n"
          ]
        }
      ],
      "source": [
        "print(f'distinct y-values--{df['label'].unique()}')\n",
        "\n",
        "# X and y -> numpy objects : to train and test the models\n",
        "X = df['text'].astype(str).values\n",
        "y = df['label']\n",
        "\n",
        "# Encode 'labels' as integers since categorical variable\n",
        "le = LabelEncoder()\n",
        "y_enc = le.fit_transform(y)\n",
        "\n",
        "# np.unique(y_enc), \n",
        "#np.unique(X)\n",
        "\n",
        "## Train-Validation-Test : 80:10:10 split, random_state=42\n",
        "#---------------------------------------------------------\n",
        "# 1. split into 80% train + 20% temp\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y_enc,\n",
        "    test_size=0.20,\n",
        "    random_state=42,\n",
        "    stratify=y_enc\n",
        ")\n",
        "\n",
        "# 2. split temp into 10% val + 10% test\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp,\n",
        "    test_size=0.50,\n",
        "    random_state=42,\n",
        "    stratify=y_temp\n",
        ")\n",
        "print(f'Total rows {df.shape[0]}, 80%of it = {df.shape[0]*0.8}, 10%of it= {df.shape[0]*0.1}')\n",
        "print(\"Train size:\", len(X_train))\n",
        "print(\"Validation size:\", len(X_val))\n",
        "print(\"Test size:\", len(X_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_KXrg5F325M"
      },
      "source": [
        "**Part 1: Bag Of Words (20 points):**\n",
        "\n",
        "* Here we are training a text classifier via *multi-class logistic regression model*.\n",
        "* Each document is represented as a binary-valued vector of dimension equal to the size of the\n",
        "vocabulary. The value at an index is 1 if the word corresponding to that index is present in the\n",
        "document, else 0.\n",
        "* We are using *macro-f1* score for evaluation.\n",
        "* Macro F1 = average of F1-scores for each class (unweighted).\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(a) **Binary Valued Vector**:\n",
        "\n",
        "Here we are finding : Does a word appear? Yes=1 , No= 0\n",
        "* 1. CountVectorizer creates a vocabulary of words and turns each document into a numerical vector.\n",
        "        Here:\n",
        "       - binary=True → 1 if the word appears, 0 otherwise (not word counts)\n",
        "       - stop_words=\"english\" → remove very common useless words like \"the\" in english language\n",
        "       - max_features=20000 → keep only the top 20,000 most common words\n",
        "* 2. Multiclass Logistic Regression:  \n",
        "    - The validation set f1-score is 0.96 and the test set f1-score is 0.97\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Binary Bag-of-Words Model: Results\n",
            "-------------------------------------\n",
            "Training shape: (9215, 60681)\n",
            "Validation shape: (1152, 60681)\n",
            "Test shape: (1152, 60681)\n",
            "\n",
            "Train set Macro F1: 1.0\n",
            "Validation Macro F1: 0.962051917357083\n",
            "Test Macro F1: 0.9754474603839395\n"
          ]
        }
      ],
      "source": [
        "# (a) binary valued vector:\n",
        "\n",
        "\n",
        "# 1. CountVectorizer:\n",
        "#-----------------------------------\n",
        "binary_vectorizer = CountVectorizer(\n",
        "    binary=True,\n",
        "    stop_words=\"english\", \n",
        "    max_features=200000\n",
        ")\n",
        "\n",
        "# 1.1: Fit the vectorizer on the TRAINING text:\n",
        "X_train_binary = binary_vectorizer.fit_transform(X_train)\n",
        "X_val_binary   = binary_vectorizer.transform(X_val)\n",
        "X_test_binary = binary_vectorizer.transform(X_test)\n",
        "\n",
        "\n",
        "# 2. LogisticRegression multi class classifier:\n",
        "#---------------------------------------------------\n",
        "model_a = LogisticRegression(\n",
        "    max_iter=1000,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# 2.1: Train the classifier using the training vectors:\n",
        "model_a.fit(X_train_binary, y_train)\n",
        "\n",
        "\n",
        "# 2.2:Predict on the train-val-test:\n",
        "y_train_pred = model_a.predict(X_train_binary)\n",
        "y_val_pred = model_a.predict(X_val_binary)\n",
        "y_test_pred = model_a.predict(X_test_binary)\n",
        "\n",
        "#2.3: Evaluate using MACRO F1:\n",
        "#-----------------------------------\n",
        "\n",
        "# Evaluation on train and Validation set:\n",
        "f1_train= f1_score(y_train,y_train_pred, average='macro')\n",
        "f1_val = f1_score(y_val, y_val_pred, average=\"macro\")\n",
        "\n",
        "# Evaluation on test set :\n",
        "f1_test = f1_score(y_test, y_test_pred, average=\"macro\")\n",
        "\n",
        "\n",
        "# RESULTS:\n",
        "#----------------------------------------\n",
        "print(\"Binary Bag-of-Words Model: Results\")\n",
        "print(\"-------------------------------------\")\n",
        "print(\"Training shape:\", X_train_binary.shape)\n",
        "print(\"Validation shape:\", X_val_binary.shape)\n",
        "print(\"Test shape:\", X_test_binary.shape)\n",
        "print(\"\\nTrain set Macro F1:\",f1_train)\n",
        "print(\"Validation Macro F1:\", f1_val)\n",
        "print(\"Test Macro F1:\", f1_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**(b) Frequency Vector**\n",
        "\n",
        "In this part we are finding : how many times a word appear?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3zw5t27sKoWx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Frequency Bag-of-Words Model: Results\n",
            "-------------------------------------\n",
            "Training shape: (9215, 60681)\n",
            "Validation shape: (1152, 60681)\n",
            "Test shape: (1152, 60681)\n",
            "\n",
            "Train set Macro F1: 1.0\n",
            "Validation Macro F1: 0.9711100702364143\n",
            "Test Macro F1: 0.9834104938271605\n"
          ]
        }
      ],
      "source": [
        "# (b) frequency vector\n",
        "# 1. Count Vectorizer:\n",
        "#-------------------------\n",
        "freq_vectorizer = CountVectorizer(\n",
        "    binary=False,          # use counts instead of 0/1\n",
        "    stop_words=\"english\",  # remove common English stop words\n",
        "    max_features=20000     # limit vocabulary size\n",
        ")\n",
        "\n",
        "# 1.1: Fit the vectorizer on the TRAINING text:\n",
        "X_train_freq = freq_vectorizer.fit_transform(X_train)\n",
        "X_val_freq   = freq_vectorizer.transform(X_val)\n",
        "X_test_freq = freq_vectorizer.transform(X_test)\n",
        "\n",
        "# 2. LogisticRegression multi class classifier :\n",
        "#---------------------------------------------------\n",
        "model_b = LogisticRegression(\n",
        "    max_iter=1000,\n",
        "    n_jobs=-1\n",
        ")\n",
        "# 2.1: Train the classifier using the training vectors:\n",
        "model_b.fit(X_train_freq, y_train)\n",
        "\n",
        "\n",
        "# 2.2:Predict on the train-val-test:\n",
        "y_train_pred = model_b.predict(X_train_freq)\n",
        "y_val_pred = model_b.predict(X_val_freq)\n",
        "y_test_pred = model_b.predict(X_test_freq)\n",
        "\n",
        "#2.3: Evaluate using MACRO F1:\n",
        "#-----------------------------------\n",
        "\n",
        "# Evaluation on train and Validation set:\n",
        "f1_train= f1_score(y_train,y_train_pred, average='macro')\n",
        "f1_val = f1_score(y_val, y_val_pred, average=\"macro\")\n",
        "\n",
        "# Evaluation on test set :\n",
        "f1_test = f1_score(y_test, y_test_pred, average=\"macro\")\n",
        "\n",
        "\n",
        "# RESULTS:\n",
        "#----------------------------------------\n",
        "print(\"Frequency Bag-of-Words Model: Results\")\n",
        "print(\"-------------------------------------\")\n",
        "print(\"Training shape:\", X_train_binary.shape)\n",
        "print(\"Validation shape:\", X_val_binary.shape)\n",
        "print(\"Test shape:\", X_test_binary.shape)\n",
        "print(\"\\nTrain set Macro F1:\",f1_train)\n",
        "print(\"Validation Macro F1:\", f1_val)\n",
        "print(\"Test Macro F1:\", f1_test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**(c) TF-IDF Vector**:\n",
        " \n",
        " Here we are finding whether a single word is important for the given document?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VX3vTT5V4Lz2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TFID Bag-of-Words Model: Results\n",
            "-------------------------------------\n",
            "Training shape: (9215, 20000)\n",
            "Validation shape: (1152, 20000)\n",
            "Test shape: (1152, 20000)\n",
            "\n",
            "Train set Macro F1: 0.986960895811587\n",
            "Validation Macro F1: 0.9591570379635157\n",
            "Test Macro F1: 0.9756933280640178\n"
          ]
        }
      ],
      "source": [
        "# (c) tf-idf vector\n",
        "# TODO\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# 1. Count Vectorizer:\n",
        "#-------------------------\n",
        "tfid_vectorizer = TfidfVectorizer(\n",
        "    #binary=False,          # use counts instead of 0/1\n",
        "    stop_words=\"english\",  # remove common English stop words\n",
        "    max_features=20000     # limit vocabulary size\n",
        ")\n",
        "\n",
        "# 1.1: Fit the vectorizer on the TRAINING text:\n",
        "X_train_tfid = tfid_vectorizer.fit_transform(X_train)\n",
        "X_val_tfid   = tfid_vectorizer.transform(X_val)\n",
        "X_test_tfid = tfid_vectorizer.transform(X_test)\n",
        "\n",
        "# 2. LogisticRegression multi class classifier :\n",
        "#---------------------------------------------------\n",
        "model_c = LogisticRegression(\n",
        "    max_iter=1000,\n",
        "    n_jobs=-1\n",
        ")\n",
        "# 2.1: Train the classifier using the training vectors:\n",
        "model_c.fit(X_train_tfid, y_train)\n",
        "\n",
        "\n",
        "# 2.2:Predict on the train-val-test:\n",
        "y_train_pred = model_c.predict(X_train_tfid)\n",
        "y_val_pred = model_c.predict(X_val_tfid)\n",
        "y_test_pred = model_c.predict(X_test_tfid)\n",
        "\n",
        "#2.3: Evaluate using MACRO F1:\n",
        "#-----------------------------------\n",
        "\n",
        "# Evaluation on train and Validation set:\n",
        "f1_train= f1_score(y_train,y_train_pred, average='macro')\n",
        "f1_val = f1_score(y_val, y_val_pred, average=\"macro\")\n",
        "\n",
        "# Evaluation on test set :\n",
        "f1_test = f1_score(y_test, y_test_pred, average=\"macro\")\n",
        "\n",
        "\n",
        "# RESULTS:\n",
        "#----------------------------------------\n",
        "print(\"TFID Bag-of-Words Model: Results\")\n",
        "print(\"-------------------------------------\")\n",
        "print(\"Training shape:\", X_train_tfid.shape)\n",
        "print(\"Validation shape:\", X_val_tfid.shape)\n",
        "print(\"Test shape:\", X_test_tfid.shape)\n",
        "print(\"\\nTrain set Macro F1:\",f1_train)\n",
        "print(\"Validation Macro F1:\", f1_val)\n",
        "print(\"Test Macro F1:\", f1_test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ja_LaTWB4NzG"
      },
      "source": [
        "Part 2: Word2Vec (20 points):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "bsN143oO4Rgy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading/Loading GloVe model...\n",
            "Model loaded!\n",
            "Vector shape: (100,)\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "# 3(i) : Pre trained Glove Model Performance:\n",
        "###-------------------------------------------\n",
        "\n",
        "\n",
        "# 1. Download the Glove model\n",
        "#----------------------------------\n",
        "import gensim.downloader as api\n",
        "\n",
        "print(\"Downloading/Loading GloVe model...\")\n",
        "# This will download the data (approx. 128MB) to your home directory\n",
        "# equivalent to glove.6B.100d.txt\n",
        "glove_model = api.load(\"glove-wiki-gigaword-100\")\n",
        "\n",
        "print(\"Model loaded!\")\n",
        "\n",
        "# Test it: Get the vector for 'king'\n",
        "vector = glove_model['king']\n",
        "print(f\"Vector shape: {vector.shape}\")  # Should be (100,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document vector shape: (9215, 100)\n",
            "\n",
            "--- GloVe 100d Results ---\n",
            "\n",
            "VALIDATION SET PERFORMANCE\n",
            "Validation Accuracy : 0.9765625\n",
            "Validation Macro F1 : 0.945777565768855\n",
            "Validation Micro F1 : 0.9765625\n",
            "\n",
            "TEST SET PERFORMANCE\n",
            "Test Accuracy : 0.9852430555555556\n",
            "Test Macro F1 : 0.9646170284241168\n",
            "Test Micro F1 : 0.9852430555555556\n"
          ]
        }
      ],
      "source": [
        "EMB_DIM = glove_model.vector_size\n",
        "\n",
        "# 2. Convert document to averaged GloVe vector\n",
        "#---------------------------------------------\n",
        "def glove_doc_vector(doc):\n",
        "    words = doc.lower().split()\n",
        "    vectors = [glove_model[w] for w in words if w in glove_model]\n",
        "    \n",
        "    if not len(vectors):  # if no words found\n",
        "        return np.zeros(EMB_DIM)\n",
        "    \n",
        "    return np.mean(vectors, axis=0)\n",
        "\n",
        "# 2.1 Build document vectors\n",
        "X_train_glove = np.vstack([glove_doc_vector(doc) for doc in X_train])\n",
        "X_val_glove   = np.vstack([glove_doc_vector(doc) for doc in X_val])\n",
        "X_test_glove  = np.vstack([glove_doc_vector(doc) for doc in X_test])\n",
        "\n",
        "print(\"Document vector shape:\", X_train_glove.shape)\n",
        "\n",
        "# 3.Logistic Regression classifier\n",
        "#-----------------------------------\n",
        "model_glove = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
        "model_glove.fit(X_train_glove, y_train)\n",
        "\n",
        "# 4. Evaluation\n",
        "#---------------------------\n",
        "# Validation scores\n",
        "y_val_pred = model_glove.predict(X_val_glove)\n",
        "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "val_macro_f1 = f1_score(y_val, y_val_pred, average=\"macro\")\n",
        "val_micro_f1 = f1_score(y_val, y_val_pred, average=\"micro\")\n",
        "\n",
        "# Test scores (final evaluation)\n",
        "y_test_pred = model_glove.predict(X_test_glove)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "test_macro_f1 = f1_score(y_test, y_test_pred, average=\"macro\")\n",
        "test_micro_f1 = f1_score(y_test, y_test_pred, average=\"micro\")\n",
        "\n",
        "print(\"\\n--- GloVe 100d Results ---\")\n",
        "print(\"\\nVALIDATION SET PERFORMANCE\")\n",
        "print(\"Validation Accuracy :\", val_accuracy)\n",
        "print(\"Validation Macro F1 :\", val_macro_f1)\n",
        "print(\"Validation Micro F1 :\", val_micro_f1)\n",
        "print(\"\\nTEST SET PERFORMANCE\")\n",
        "print(\"Test Accuracy :\", test_accuracy)\n",
        "print(\"Test Macro F1 :\", test_macro_f1)\n",
        "print(\"Test Micro F1 :\", test_micro_f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
            "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W2V document vector shape: (9215, 100)\n",
            "\n",
            "--- Word2Vec (trained on NYT) Results ---\n",
            "Validation Accuracy : 0.9644097222222222\n",
            "Validation Macro F1 : 0.9227649132845706\n",
            "Validation Micro F1 : 0.9644097222222222\n",
            "\n",
            "TEST SET PERFORMANCE\n",
            "Test Accuracy : 0.9791666666666666\n",
            "Test Macro F1 : 0.948267861342574\n",
            "Test Micro F1 : 0.9791666666666666\n"
          ]
        }
      ],
      "source": [
        "# 3(ii) Word2Vec Model :\n",
        "##--------------------------------------------\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download tokenizer resource if not present\n",
        "#nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "\n",
        "# 1. Create tokenized sentences from the training text\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# Manual tokenizer using regex to remove punctuation\n",
        "# def tokenize(doc):\n",
        "#     return re.findall(r\"\\b[a-zA-Z]+\\b\", doc.lower())\n",
        "#train_sentences = [tokenize(doc) for doc in X_train]\n",
        "\n",
        "train_sentences=[word_tokenize(doc) for doc in X_train]\n",
        "\n",
        "# 2. Train Word2Vec model\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "w2v_model = Word2Vec(\n",
        "    sentences=train_sentences,\n",
        "    vector_size=100,   # 100-dimensional as required\n",
        "    window=5,          # context window\n",
        "    min_count=2,       # ignore very rare words\n",
        "    workers=4,         # parallel training\n",
        "    sg=1               # sg=1 : SkipGram (better for quality)\n",
        ")\n",
        "\n",
        "EMB_DIM = 100\n",
        "\n",
        "\n",
        "# 3. Convert documents to averaged W2V vectors\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "def w2v_doc_vector(doc):\n",
        "    #tokens = tokenize(doc)\n",
        "    tokens=word_tokenize(doc)\n",
        "    vectors = [w2v_model.wv[w] for w in tokens if w in w2v_model.wv] #Access the vectors via .wv\n",
        "    \n",
        "    if not vectors:\n",
        "        return np.zeros(EMB_DIM)\n",
        "    \n",
        "    return np.mean(vectors, axis=0)\n",
        "\n",
        "# Build vectors for train/validation/test\n",
        "X_train_w2v = np.vstack([w2v_doc_vector(doc) for doc in X_train])\n",
        "X_val_w2v   = np.vstack([w2v_doc_vector(doc) for doc in X_val])\n",
        "X_test_w2v  = np.vstack([w2v_doc_vector(doc) for doc in X_test])\n",
        "\n",
        "print(\"W2V document vector shape:\", X_train_w2v.shape)\n",
        "\n",
        "\n",
        "# 4. Train Logistic Regression classifier\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "model_w2v = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
        "model_w2v.fit(X_train_w2v, y_train)\n",
        "\n",
        "\n",
        "# 5. Evaluation:\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "y_val_pred = model_w2v.predict(X_val_w2v)\n",
        "\n",
        "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "val_macro_f1 = f1_score(y_val, y_val_pred, average=\"macro\")\n",
        "val_micro_f1 = f1_score(y_val, y_val_pred, average=\"micro\")\n",
        "\n",
        "\n",
        "\n",
        "y_test_pred = model_w2v.predict(X_test_w2v)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "test_macro_f1 = f1_score(y_test, y_test_pred, average=\"macro\")\n",
        "test_micro_f1 = f1_score(y_test, y_test_pred, average=\"micro\")\n",
        "\n",
        "print(\"\\n--- Word2Vec (trained on NYT) Results ---\")\n",
        "print(\"Validation Accuracy :\", val_accuracy)\n",
        "print(\"Validation Macro F1 :\", val_macro_f1)\n",
        "print(\"Validation Micro F1 :\", val_micro_f1)\n",
        "\n",
        "print(\"\\nTEST SET PERFORMANCE\")\n",
        "print(\"Test Accuracy :\", test_accuracy)\n",
        "print(\"Test Macro F1 :\", test_macro_f1)\n",
        "print(\"Test Micro F1 :\", test_micro_f1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5o0icLb4R8-"
      },
      "source": [
        "Part 3: BERT (20 points):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_-H1VIh4WCM"
      },
      "outputs": [],
      "source": [
        "# Packages for Bert:\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "23d1fa0367f441c8bf4a1b5157e7b072",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6a585e5fb3f84c0eaf9f9af17a2dc6ba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd5ab26ec1194c9ea89f061741f3ea41",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba3d671896bf43d492d175127fa3d126",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "# 1. Set Parameters:\n",
        "#-----------------------\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "MAX_LEN = 64\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 3\n",
        "NUM_LABELS = len(set(y_train))  # typically 3\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b4724b8d41014c8ea2cf2b613c6d79b6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# 2. Dataset Class:\n",
        "#----------------------------------\n",
        "class NYTDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = int(self.labels[idx])\n",
        "        \n",
        "        enc = tokenizer(\n",
        "            text,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=MAX_LEN,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            \"input_ids\": enc[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask\": enc[\"attention_mask\"].squeeze(),\n",
        "            \"labels\": torch.tensor(label)\n",
        "        }\n",
        "\n",
        "\n",
        "# 3. Data Loader:\n",
        "#------------------------------\n",
        "train_ds = NYTDataset(X_train, y_train)\n",
        "val_ds   = NYTDataset(X_val, y_val)\n",
        "test_ds  = NYTDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
        "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
        "\n",
        "# 4. Model + Optimizer:\n",
        "#------------------------------\n",
        "model_bert = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels=NUM_LABELS\n",
        ")\n",
        "model_bert = model_bert.to(device)\n",
        "\n",
        "optimizer = AdamW(model_bert.parameters(), lr=2e-5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/3: 100%|██████████| 576/576 [05:31<00:00,  1.74it/s, loss=0.00343]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 completed. Avg Loss = 0.1400\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/3: 100%|██████████| 576/576 [05:44<00:00,  1.67it/s, loss=0.025]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 completed. Avg Loss = 0.0428\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/3: 100%|██████████| 576/576 [05:49<00:00,  1.65it/s, loss=0.00579] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 completed. Avg Loss = 0.0182\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# 5. Train the model : Epoch size =3\n",
        "#---------------------------------------\n",
        "for epoch in range(EPOCHS):\n",
        "    model_bert.train()\n",
        "    loop = tqdm(train_loader, leave=True)\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in loop:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        \n",
        "        model_bert.zero_grad()\n",
        "        outputs = model_bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "        \n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        loop.set_description(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "    \n",
        "    print(f\"Epoch {epoch+1} completed. Avg Loss = {total_loss / len(loop):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Results\n",
            "---------------------------\n",
            "Accuracy : 0.9774305555555556\n",
            "Macro F1 : 0.9527343723887988\n",
            "Micro F1 : 0.9774305555555556\n"
          ]
        }
      ],
      "source": [
        "# Evaluation Function:\n",
        "def evaluate(loader):\n",
        "    model_bert.eval()\n",
        "    preds, labels_all = [], []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].cpu().numpy()\n",
        "            \n",
        "            outputs = model_bert(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask\n",
        "            )\n",
        "            \n",
        "            batch_preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
        "            \n",
        "            preds.extend(batch_preds)\n",
        "            labels_all.extend(labels)\n",
        "    \n",
        "    acc = accuracy_score(labels_all, preds)\n",
        "    macro = f1_score(labels_all, preds, average=\"macro\")\n",
        "    micro = f1_score(labels_all, preds, average=\"micro\")\n",
        "    \n",
        "    return acc, macro, micro\n",
        "\n",
        "# Validation Set :\n",
        "val_acc, val_macro, val_micro = evaluate(val_loader)\n",
        "\n",
        "print(\"Validation Results\")\n",
        "print(\"---------------------------\")\n",
        "print(\"Accuracy :\", val_acc)\n",
        "print(\"Macro F1 :\", val_macro)\n",
        "print(\"Micro F1 :\", val_micro)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "TEST RESULTS\n",
            "---------------------------\n",
            "Accuracy : 0.984375\n",
            "Macro F1 : 0.9651937934318485\n",
            "Micro F1 : 0.984375\n"
          ]
        }
      ],
      "source": [
        "test_acc, test_macro, test_micro = evaluate(test_loader)\n",
        "\n",
        "print(\"\\nTEST RESULTS\")\n",
        "print(\"---------------------------\")\n",
        "print(\"Accuracy :\", test_acc)\n",
        "print(\"Macro F1 :\", test_macro)\n",
        "print(\"Micro F1 :\", test_micro)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYqEGP9D4ZX7"
      },
      "source": [
        "**Part 4: Summary of results / Reflection**\n",
        "* Part 1:\n",
        "  \n",
        "| Model                   | Validation Macro F1  | Test Macro F1  |\n",
        "| ----------------------- | -------------------- | --------------- |\n",
        "| **Binary Bag-of-Words**     | 0.962                |  0.975           |\n",
        "| **Frequency Bag-of-words**  | 0.971                | 0.983            |\n",
        "| **TFID Bag-of-Words**       | 0.959                | 0.976            |\n",
        "\n",
        "* Part 2 a:\n",
        "\n",
        "|**Glove Model (Pretrained)** | Validation Set | Test Set  |\n",
        "|----------------------   |----------------|-----------|\n",
        "|Accuracy score           | 0.977          |0.985       |\n",
        "|Macro F1                 | 0.946          | 0.965      |\n",
        "|Micro F1                 | 0.978          | 0.985      |\n",
        "\n",
        "* Part 2 b:\n",
        "\n",
        "\n",
        "|**Word2Vec (Trained on NYT)** | Validation Set | Test Set  |\n",
        "|----------------------   |----------------|-----------|\n",
        "|Accuracy score           | 0.964         |0.979     |\n",
        "|Macro F1                 | 0.922         | 0.948     |\n",
        "|Micro F1                 | 0.964         | 0.979     |\n",
        "\n",
        "* Part 3:\n",
        "\n",
        "|**BERT (Trained on NYT)** | Validation Set | Test Set  |\n",
        "|----------------------   |----------------|-----------|\n",
        "|Accuracy score           | 0.977          |0.984      |\n",
        "|Macro F1                 | 0.952          | 0.965      |\n",
        "|Micro F1                 | 0.977          | 0.984      |\n",
        "\n",
        "\n",
        "* **Summary**:\n",
        "\n",
        "\n",
        "  The results show that all models performed extremely well on the NYT dataset, largely because the article categories (business, sports, politics) contain highly distinctive and non-overlapping vocabulary. As a result, even very simple representations such as Binary Bag-of-Words and Frequency Bag-of-Words achieved near-perfect macro F1 scores (≈0.97–0.98).TF–IDF also performed similarly well.The pretrained GloVe embeddings also achieved strong performance, benefiting from global semantic information, while the Word2Vec model trained solely on the NYT dataset performed slightly worse due to the relatively small corpus size. BERT, despite being a powerful transformer-based model, provided only marginal improvements over classical models because the task does not require deep contextual reasoning, the vocabulary alone is sufficient to distinguish the categories. Overall, the experiment highlights that for clean topic-classification datasets, traditional vector-space models can perform as well as or even better than modern deep models like BERT.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "data-sc (3.12.10)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
