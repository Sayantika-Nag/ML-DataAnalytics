{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1IzJEvsxyW0UTxMRxS_TkhDdV_Eur26d1",
      "authorship_tag": "ABX9TyORPs73rmNrSnGBPmrrN7Ch"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Goal of the Assignment**\n",
        "\n",
        "We are trying to build a binary classifier neural network to detect:\n",
        "\n",
        "  - Real cats have label = 0\n",
        "\n",
        "  - Fake cats have label = 1\n",
        "\n",
        "Images are:\n",
        "\n",
        "- Size: 32 Ã— 32\n",
        "\n",
        "- Format: RGB\n",
        "\n",
        "- Tensor shape: [N, 3, 32, 32]"
      ],
      "metadata": {
        "id": "8sId-_zoupCS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U74-7yz1spMe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
        "\n",
        "import torchvision.utils as vutils\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torch.optim import SGD, Adam # optimizer\n",
        "\n",
        "\n",
        "from PIL import Image\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Data:"
      ],
      "metadata": {
        "id": "f_fbwcApwj8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Loading dataset\")\n",
        "X_raw, Y_raw = torch.load(\"/content/drive/MyDrive/Colab Notebooks/deepfake detector/hw2_data.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qq9NvO9BwjY_",
        "outputId": "3316d1e6-9cf3-4756-9b34-ac94835448a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Data shapes--{X_raw.shape}, {Y_raw.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUS5lXb7xawe",
        "outputId": "0d570e38-908d-4022-bff2-89be192830d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data shapes--torch.Size([2000, 3, 32, 32]), torch.Size([2000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline Model:"
      ],
      "metadata": {
        "id": "6Q8eiSZqwc2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A. Feature Extraction via ResNet() backbone:\n",
        "\n",
        "# 1. Load the pretrained model:\n",
        "resnet = models.resnet18(pretrained=True)\n",
        "resnet.fc = nn.Identity() # output embeddings instead of logits\n",
        "resnet.eval() # Put network in evaluation mode, dropout is switched off\n",
        "\n",
        "# 2. Function to extract features:\n",
        "def extract_features(images, batch_size=100):\n",
        "    \"\"\"Runs images through ResNet18 to get 512-dim embeddings.\"\"\"\n",
        "    embeddings = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(images), batch_size):\n",
        "            batch = images[i: i+batch_size]\n",
        "            emb = resnet(batch)\n",
        "            embeddings.append(emb)\n",
        "    return torch.cat(embeddings)\n",
        "\n",
        "# 2.2 Extract baseline features\n",
        "print(\"Extracting baseline features...\")\n",
        "X_emb = extract_features(X_raw)  # new data for training the model\n",
        "print(f\"Features extracted. Baseline features shape: {X_emb.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5j1UXb-zv54c",
        "outputId": "22f3e3b9-745a-4099-a598-6640e77a3aa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting baseline features...\n",
            "Features extracted. Baseline features shape: torch.Size([2000, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# B. MLP architecture:\n",
        "\n",
        "class MLPClassifier(nn.Module):\n",
        "    def __init__(self, input_dim=512, n_hidden=128):\n",
        "        super(MLPClassifier, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_dim, n_hidden),  # First hidden layer\n",
        "            nn.ReLU(),                       # ReLU nonlinearity\n",
        "            nn.Linear(n_hidden, n_hidden),   # Second hidden layer\n",
        "            nn.ReLU(),                       # ReLU nonlinearity\n",
        "            nn.Linear(n_hidden, 2)           # Output logits\n",
        "                                             #dimension n_classes = 2\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)  # pytorch does the forward feed by itself"
      ],
      "metadata": {
        "id": "QSK6oYbU0W4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check : Inspect this model - parameters, layers\n",
        "model = MLPClassifier()\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYvaD4su5-2W",
        "outputId": "386a62ec-e33d-4033-e95e-befd55b22468"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLPClassifier(\n",
            "  (network): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=128, out_features=2, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check : Run it - get logits\n",
        "x = torch.randn((10, 512))\n",
        "with torch.no_grad():\n",
        "    logits = model(x)\n",
        "logits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2adIS8-E8mXs",
        "outputId": "52255dcb-59d4-44d0-a7b0-8ce7dc51e0c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0910,  0.0088],\n",
              "        [-0.0766,  0.1015],\n",
              "        [-0.0369,  0.0287],\n",
              "        [-0.1319, -0.0204],\n",
              "        [-0.1197, -0.0330],\n",
              "        [-0.1069,  0.0661],\n",
              "        [-0.0445,  0.0245],\n",
              "        [-0.0911, -0.0019],\n",
              "        [-0.0609, -0.0302],\n",
              "        [-0.1336,  0.1210]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# C. Training the model with Cross Entropy loss(): W/O hyperparameter tuning\n",
        "\n",
        "# Define the model and the optimizer and the loss criterion:\n",
        "lr = 0.01; wd = 0.0001\n",
        "model = MLPClassifier()  # init model\n",
        "optimizer = SGD(model.parameters(), lr=lr, weight_decay=wd)\n",
        "loss_fn = nn.CrossEntropyLoss()  # define loss\n",
        "\n",
        "\n",
        "# Load the Tensor Dataset:\n",
        "#full_dataset = TensorDataset(X_emb, Y_raw)\n",
        "train_dataloader = DataLoader(TensorDataset(X_emb[:1500],Y_raw[:1500]), batch_size=50, shuffle=True)\n",
        "val_dataloader = DataLoader(TensorDataset(X_emb[1500:],Y_raw[1500:]), batch_size=50, shuffle=True)  # validation data!\n"
      ],
      "metadata": {
        "id": "2sEmtGGS87Tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model:\n",
        "n_epochs=50\n",
        "for i in range(n_epochs):\n",
        "    running_loss = 0.0\n",
        "    model.train()\n",
        "    for x_batch, y_batch in train_dataloader:  # 1 epoch\n",
        "        optimizer.zero_grad()  # important to zero out the gradient buffer first\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(x_batch)  # 1st step of forward - predict something\n",
        "        loss = loss_fn(logits, y_batch)  # 2nd step - get loss, comparing prediction to ground truth\n",
        "\n",
        "        # Backward pass - gradient of loss wrt parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Critical part: update weights!\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() / len(train_dataloader)\n",
        "\n",
        "    # Calculate the loss on VALIDATION data\n",
        "    model.eval()\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    val_loss = 0.\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch in val_dataloader:\n",
        "            # Forward pass\n",
        "            logits = model(x_batch)  # 1st step of forward - predict something\n",
        "            loss = loss_fn(logits, y_batch)  # 2nd step - get loss, comparing prediction to ground truth\n",
        "            val_loss += loss.item() / len(val_dataloader)\n",
        "\n",
        "            # Accuracy Calculation:\n",
        "            # Get the predicted class (0 or 1) by finding the max logit\n",
        "            predictions = torch.argmax(logits, dim=1)\n",
        "            # Update the counters\n",
        "            correct_predictions += (predictions == y_batch).sum().item()\n",
        "            total_predictions += y_batch.size(0)\n",
        "\n",
        "    # Calculate final accuracy as a ratio\n",
        "    val_accuracy = correct_predictions / total_predictions\n",
        "    print(f\"epoch {i} \\t Training loss={running_loss:.3f} || Validation loss={val_loss:.3f}|| Validation Accuracy ={val_accuracy:0.3f}\") # Loss is decresing\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_lgBkJlZogB",
        "outputId": "f152073e-214c-4ad2-be86-888f78d618d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 \t Training loss=0.006 || Validation loss=0.406|| Validation Accuracy =0.882\n",
            "epoch 1 \t Training loss=0.006 || Validation loss=0.407|| Validation Accuracy =0.882\n",
            "epoch 2 \t Training loss=0.006 || Validation loss=0.409|| Validation Accuracy =0.880\n",
            "epoch 3 \t Training loss=0.006 || Validation loss=0.413|| Validation Accuracy =0.884\n",
            "epoch 4 \t Training loss=0.006 || Validation loss=0.416|| Validation Accuracy =0.884\n",
            "epoch 5 \t Training loss=0.006 || Validation loss=0.411|| Validation Accuracy =0.882\n",
            "epoch 6 \t Training loss=0.006 || Validation loss=0.412|| Validation Accuracy =0.882\n",
            "epoch 7 \t Training loss=0.006 || Validation loss=0.412|| Validation Accuracy =0.882\n",
            "epoch 8 \t Training loss=0.006 || Validation loss=0.420|| Validation Accuracy =0.884\n",
            "epoch 9 \t Training loss=0.006 || Validation loss=0.415|| Validation Accuracy =0.880\n",
            "epoch 10 \t Training loss=0.006 || Validation loss=0.412|| Validation Accuracy =0.884\n",
            "epoch 11 \t Training loss=0.006 || Validation loss=0.414|| Validation Accuracy =0.882\n",
            "epoch 12 \t Training loss=0.005 || Validation loss=0.415|| Validation Accuracy =0.882\n",
            "epoch 13 \t Training loss=0.005 || Validation loss=0.416|| Validation Accuracy =0.882\n",
            "epoch 14 \t Training loss=0.005 || Validation loss=0.418|| Validation Accuracy =0.880\n",
            "epoch 15 \t Training loss=0.005 || Validation loss=0.419|| Validation Accuracy =0.880\n",
            "epoch 16 \t Training loss=0.005 || Validation loss=0.418|| Validation Accuracy =0.880\n",
            "epoch 17 \t Training loss=0.005 || Validation loss=0.418|| Validation Accuracy =0.880\n",
            "epoch 18 \t Training loss=0.005 || Validation loss=0.419|| Validation Accuracy =0.882\n",
            "epoch 19 \t Training loss=0.005 || Validation loss=0.418|| Validation Accuracy =0.880\n",
            "epoch 20 \t Training loss=0.005 || Validation loss=0.422|| Validation Accuracy =0.880\n",
            "epoch 21 \t Training loss=0.005 || Validation loss=0.420|| Validation Accuracy =0.882\n",
            "epoch 22 \t Training loss=0.005 || Validation loss=0.425|| Validation Accuracy =0.880\n",
            "epoch 23 \t Training loss=0.005 || Validation loss=0.425|| Validation Accuracy =0.882\n",
            "epoch 24 \t Training loss=0.005 || Validation loss=0.424|| Validation Accuracy =0.880\n",
            "epoch 25 \t Training loss=0.005 || Validation loss=0.423|| Validation Accuracy =0.882\n",
            "epoch 26 \t Training loss=0.005 || Validation loss=0.424|| Validation Accuracy =0.882\n",
            "epoch 27 \t Training loss=0.005 || Validation loss=0.426|| Validation Accuracy =0.882\n",
            "epoch 28 \t Training loss=0.005 || Validation loss=0.431|| Validation Accuracy =0.884\n",
            "epoch 29 \t Training loss=0.005 || Validation loss=0.430|| Validation Accuracy =0.880\n",
            "epoch 30 \t Training loss=0.005 || Validation loss=0.429|| Validation Accuracy =0.880\n",
            "epoch 31 \t Training loss=0.005 || Validation loss=0.430|| Validation Accuracy =0.880\n",
            "epoch 32 \t Training loss=0.004 || Validation loss=0.432|| Validation Accuracy =0.880\n",
            "epoch 33 \t Training loss=0.004 || Validation loss=0.429|| Validation Accuracy =0.880\n",
            "epoch 34 \t Training loss=0.004 || Validation loss=0.428|| Validation Accuracy =0.882\n",
            "epoch 35 \t Training loss=0.004 || Validation loss=0.430|| Validation Accuracy =0.880\n",
            "epoch 36 \t Training loss=0.004 || Validation loss=0.434|| Validation Accuracy =0.884\n",
            "epoch 37 \t Training loss=0.004 || Validation loss=0.432|| Validation Accuracy =0.880\n",
            "epoch 38 \t Training loss=0.004 || Validation loss=0.431|| Validation Accuracy =0.880\n",
            "epoch 39 \t Training loss=0.004 || Validation loss=0.438|| Validation Accuracy =0.884\n",
            "epoch 40 \t Training loss=0.004 || Validation loss=0.433|| Validation Accuracy =0.882\n",
            "epoch 41 \t Training loss=0.004 || Validation loss=0.434|| Validation Accuracy =0.880\n",
            "epoch 42 \t Training loss=0.004 || Validation loss=0.437|| Validation Accuracy =0.880\n",
            "epoch 43 \t Training loss=0.004 || Validation loss=0.436|| Validation Accuracy =0.880\n",
            "epoch 44 \t Training loss=0.004 || Validation loss=0.437|| Validation Accuracy =0.880\n",
            "epoch 45 \t Training loss=0.004 || Validation loss=0.439|| Validation Accuracy =0.880\n",
            "epoch 46 \t Training loss=0.004 || Validation loss=0.436|| Validation Accuracy =0.882\n",
            "epoch 47 \t Training loss=0.004 || Validation loss=0.439|| Validation Accuracy =0.880\n",
            "epoch 48 \t Training loss=0.004 || Validation loss=0.438|| Validation Accuracy =0.882\n",
            "epoch 49 \t Training loss=0.004 || Validation loss=0.442|| Validation Accuracy =0.880\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Model_2 =  Baseline Model + K fold Hyperparameter tuning:\n",
        "\n",
        "- Test data Accuracy 0.922"
      ],
      "metadata": {
        "id": "jEBe_Z2j8333"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# D. Model with Hyper parameter tuning with K-fold:\n",
        "\n",
        "# Hyperparameter Grid:\n",
        "learning_rates = [0.001, 0.01, 0.05, 0.1]\n",
        "weight_decays = [0, 1e-4, 1e-3, 1e-2]\n",
        "print(\"Hyper parameter Grid\")\n",
        "print(f\"Learning Rates: {learning_rates}\")\n",
        "print(f\"Weight Decay: {weight_decays}\")\n",
        "\n",
        "n_epochs=50\n",
        "\n",
        "# Prepare K-Fold:\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42) # 5-fold CV with shuffling\n",
        "full_dataset = TensorDataset(X_emb, Y_raw)\n",
        "\n",
        "# counters for best parameter update:\n",
        "best_acc = 0\n",
        "best_params = (None, None)\n",
        "\n",
        "print(\"--------------------\")\n",
        "print(\"Starting Grid Search CV...\")\n",
        "for lr in learning_rates:\n",
        "    for wd in weight_decays:\n",
        "\n",
        "        #print(\"---------------------\")\n",
        "        #print(f\"\\nLearning Rate: {lr}, Weight Decay: {wd}\")\n",
        "\n",
        "        fold_accuracies = [] # List of fold accuracies\n",
        "        train_loss=[]\n",
        "        validation_loss=[]\n",
        "        #fold=0\n",
        "        for train_idx, val_idx in kf.split(full_dataset):\n",
        "            #print(f\" \\nFold {fold}\")\n",
        "            #fold+=1\n",
        "\n",
        "            # Split data\n",
        "            train_sub = Subset(full_dataset, train_idx)\n",
        "            val_sub = Subset(full_dataset, val_idx)\n",
        "\n",
        "            train_loader = DataLoader(train_sub, batch_size=50, shuffle=True)\n",
        "            val_loader = DataLoader(val_sub, batch_size=50, shuffle=True)\n",
        "\n",
        "            # Initialize Model, Loss, Optimizer\n",
        "            model = MLPClassifier()\n",
        "            loss_fn = nn.CrossEntropyLoss() # Use Cross Entropy loss\n",
        "            optimizer =SGD(model.parameters(), lr=lr, weight_decay=wd) # SGD optimizer\n",
        "\n",
        "            # Train for n_epochs\n",
        "            model.train()\n",
        "\n",
        "            for epoch in range(n_epochs):\n",
        "                running_loss = 0.0\n",
        "                for batch_X, batch_y in train_loader:\n",
        "                    optimizer.zero_grad()\n",
        "                    logits = model(batch_X)\n",
        "                    loss = loss_fn(logits, batch_y)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    running_loss += loss.item() / len(train_loader)\n",
        "            #print(f\"training loss = {running_loss : 0.4f}\")\n",
        "            train_loss.append(running_loss)\n",
        "            # Validate\n",
        "            model.eval()\n",
        "            correct_predictions=0\n",
        "            total_predictions = 0\n",
        "            val_loss=0.0\n",
        "            with torch.no_grad():\n",
        "                for batch_X, batch_y in val_loader:\n",
        "                    logits = model(batch_X)\n",
        "                    loss = loss_fn(logits, batch_y)\n",
        "                    val_loss += loss.item() / len(val_loader)\n",
        "                    predictions = torch.argmax(logits, dim=1)\n",
        "                    correct_predictions += (predictions == batch_y).sum().item()\n",
        "                    total_predictions += batch_y.size(0)\n",
        "                val_acc= correct_predictions/total_predictions\n",
        "            #print(f\"validation loss = {val_loss : 0.4f}\")\n",
        "            #print(f\"validation accuracy = {val_acc : 0.4f}\")\n",
        "\n",
        "            validation_loss.append(val_loss)\n",
        "            #fold_accuracies.append(correct_predictions / total_predictions)\n",
        "            fold_accuracies.append(val_acc)\n",
        "\n",
        "        mean_val_acc = np.mean(fold_accuracies) # Choose hyperparameters by mean validation accuracy\n",
        "        mean_train_loss=np.mean(train_loss)\n",
        "        mean_val_loss=np.mean(validation_loss)\n",
        "\n",
        "        print(\"-----------\")\n",
        "        print(f\"Learning Rate: {lr}, Weight Decay: {wd}\")\n",
        "        print(f\"Train_Loss ={mean_train_loss:0.4f}|| Validation_loss ={mean_val_loss:0.4f}|| Validation_accurancy ={mean_val_acc:.4f},\")\n",
        "\n",
        "        if mean_val_acc > best_acc:\n",
        "            best_acc = mean_val_acc\n",
        "            best_params = (lr, wd)\n",
        "print(\"\\n------------------------------\")\n",
        "print(f\"Best Parameters: Learning Rate={best_params[0]}, Weight Decay={best_params[1]} with Validation Accuracy={best_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojKgB99NdenL",
        "outputId": "ade37421-8b9e-435d-cbd1-ac8e0b7348f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyper parameter Grid\n",
            "Learning Rates: [0.001, 0.01, 0.05, 0.1]\n",
            "Weight Decay: [0, 0.0001, 0.001, 0.01]\n",
            "--------------------\n",
            "Starting Grid Search CV...\n",
            "-----------\n",
            "Learning Rate: 0.001, Weight Decay: 0\n",
            "Train_Loss =0.5145|| Validation_loss =0.5179|| Validation_accurancy =0.8110,\n",
            "-----------\n",
            "Learning Rate: 0.001, Weight Decay: 0.0001\n",
            "Train_Loss =0.5171|| Validation_loss =0.5174|| Validation_accurancy =0.8110,\n",
            "-----------\n",
            "Learning Rate: 0.001, Weight Decay: 0.001\n",
            "Train_Loss =0.5226|| Validation_loss =0.5276|| Validation_accurancy =0.8145,\n",
            "-----------\n",
            "Learning Rate: 0.001, Weight Decay: 0.01\n",
            "Train_Loss =0.5349|| Validation_loss =0.5377|| Validation_accurancy =0.8105,\n",
            "-----------\n",
            "Learning Rate: 0.01, Weight Decay: 0\n",
            "Train_Loss =0.1297|| Validation_loss =0.2254|| Validation_accurancy =0.9115,\n",
            "-----------\n",
            "Learning Rate: 0.01, Weight Decay: 0.0001\n",
            "Train_Loss =0.1355|| Validation_loss =0.2273|| Validation_accurancy =0.9135,\n",
            "-----------\n",
            "Learning Rate: 0.01, Weight Decay: 0.001\n",
            "Train_Loss =0.1367|| Validation_loss =0.2202|| Validation_accurancy =0.9140,\n",
            "-----------\n",
            "Learning Rate: 0.01, Weight Decay: 0.01\n",
            "Train_Loss =0.1451|| Validation_loss =0.2223|| Validation_accurancy =0.9095,\n",
            "-----------\n",
            "Learning Rate: 0.05, Weight Decay: 0\n",
            "Train_Loss =0.0098|| Validation_loss =0.3148|| Validation_accurancy =0.9135,\n",
            "-----------\n",
            "Learning Rate: 0.05, Weight Decay: 0.0001\n",
            "Train_Loss =0.0113|| Validation_loss =0.3174|| Validation_accurancy =0.9160,\n",
            "-----------\n",
            "Learning Rate: 0.05, Weight Decay: 0.001\n",
            "Train_Loss =0.0153|| Validation_loss =0.3019|| Validation_accurancy =0.9135,\n",
            "-----------\n",
            "Learning Rate: 0.05, Weight Decay: 0.01\n",
            "Train_Loss =0.0840|| Validation_loss =0.2602|| Validation_accurancy =0.9130,\n",
            "-----------\n",
            "Learning Rate: 0.1, Weight Decay: 0\n",
            "Train_Loss =0.0069|| Validation_loss =0.3731|| Validation_accurancy =0.9080,\n",
            "-----------\n",
            "Learning Rate: 0.1, Weight Decay: 0.0001\n",
            "Train_Loss =0.0027|| Validation_loss =0.3627|| Validation_accurancy =0.9180,\n",
            "-----------\n",
            "Learning Rate: 0.1, Weight Decay: 0.001\n",
            "Train_Loss =0.0038|| Validation_loss =0.3523|| Validation_accurancy =0.9180,\n",
            "-----------\n",
            "Learning Rate: 0.1, Weight Decay: 0.01\n",
            "Train_Loss =0.1524|| Validation_loss =0.2330|| Validation_accurancy =0.9140,\n",
            "\n",
            "------------------------------\n",
            "Best Parameters: Learning Rate=0.1, Weight Decay=0.0001 with Validation Accuracy=0.9180\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Model after Hyperparameter training\n",
        "\n",
        "- I chose the best parameters:\n",
        "\n",
        "   **Learning Rate=0.1, Weight Decay=0.0001**\n",
        "   with Validation Accuracy=0.9180 OR 91.8%\n",
        "\n",
        "- I trained the full dataset on this model and predicted the labels for the test data set.   "
      ],
      "metadata": {
        "id": "BjWv9rp9_MZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Final training on all data :\n",
        "full_dataset = TensorDataset(X_emb, Y_raw)\n",
        "full_loader = DataLoader(full_dataset, batch_size=50, shuffle=True)\n",
        "\n",
        "# Initialize the final model with best hyperparameters:\n",
        "final_model = MLPClassifier() # Create a new instance of the model\n",
        "loss_fn = nn.CrossEntropyLoss() # Define loss function\n",
        "final_optimizer = SGD(final_model.parameters(), lr=best_params[0], weight_decay=best_params[1])\n",
        "\n",
        "# Train the final model on the entire dataset\n",
        "n_epochs = 50\n",
        "print(f\"Training final model with LR={best_params[0]} and WD={best_params[1]} for {n_epochs} epochs\")\n",
        "print(\"------------\")\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    final_model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for x_batch, y_batch in full_loader:\n",
        "        final_optimizer.zero_grad()\n",
        "        logits = final_model(x_batch)\n",
        "        loss = loss_fn(logits, y_batch)\n",
        "        loss.backward()\n",
        "        final_optimizer.step()\n",
        "        running_loss += loss.item() / len(full_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch} \\t Training Loss: {running_loss:.4f}\")\n",
        "\n",
        "print(\"Final model training complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Il-5I-2m_4AY",
        "outputId": "51cf52ff-f6d2-49e9-c3a8-c6a30b74acc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training final model with LR=0.1 and WD=0.0001 for 50 epochs\n",
            "------------\n",
            "Epoch 0 \t Training Loss: 0.5710\n",
            "Epoch 1 \t Training Loss: 0.3809\n",
            "Epoch 2 \t Training Loss: 0.3064\n",
            "Epoch 3 \t Training Loss: 0.2759\n",
            "Epoch 4 \t Training Loss: 0.2415\n",
            "Epoch 5 \t Training Loss: 0.2225\n",
            "Epoch 6 \t Training Loss: 0.2371\n",
            "Epoch 7 \t Training Loss: 0.1970\n",
            "Epoch 8 \t Training Loss: 0.2406\n",
            "Epoch 9 \t Training Loss: 0.1730\n",
            "Epoch 10 \t Training Loss: 0.1564\n",
            "Epoch 11 \t Training Loss: 0.1610\n",
            "Epoch 12 \t Training Loss: 0.1502\n",
            "Epoch 13 \t Training Loss: 0.1632\n",
            "Epoch 14 \t Training Loss: 0.1234\n",
            "Epoch 15 \t Training Loss: 0.1649\n",
            "Epoch 16 \t Training Loss: 0.1380\n",
            "Epoch 17 \t Training Loss: 0.0961\n",
            "Epoch 18 \t Training Loss: 0.1080\n",
            "Epoch 19 \t Training Loss: 0.0857\n",
            "Epoch 20 \t Training Loss: 0.1138\n",
            "Epoch 21 \t Training Loss: 0.0797\n",
            "Epoch 22 \t Training Loss: 0.0484\n",
            "Epoch 23 \t Training Loss: 0.0585\n",
            "Epoch 24 \t Training Loss: 0.0986\n",
            "Epoch 25 \t Training Loss: 0.0802\n",
            "Epoch 26 \t Training Loss: 0.0365\n",
            "Epoch 27 \t Training Loss: 0.0275\n",
            "Epoch 28 \t Training Loss: 0.0299\n",
            "Epoch 29 \t Training Loss: 0.0191\n",
            "Epoch 30 \t Training Loss: 0.0148\n",
            "Epoch 31 \t Training Loss: 0.0116\n",
            "Epoch 32 \t Training Loss: 0.0089\n",
            "Epoch 33 \t Training Loss: 0.0083\n",
            "Epoch 34 \t Training Loss: 0.0056\n",
            "Epoch 35 \t Training Loss: 0.0045\n",
            "Epoch 36 \t Training Loss: 0.0038\n",
            "Epoch 37 \t Training Loss: 0.0037\n",
            "Epoch 38 \t Training Loss: 0.0028\n",
            "Epoch 39 \t Training Loss: 0.0025\n",
            "Epoch 40 \t Training Loss: 0.0023\n",
            "Epoch 41 \t Training Loss: 0.0021\n",
            "Epoch 42 \t Training Loss: 0.0020\n",
            "Epoch 43 \t Training Loss: 0.0018\n",
            "Epoch 44 \t Training Loss: 0.0017\n",
            "Epoch 45 \t Training Loss: 0.0016\n",
            "Epoch 46 \t Training Loss: 0.0014\n",
            "Epoch 47 \t Training Loss: 0.0014\n",
            "Epoch 48 \t Training Loss: 0.0013\n",
            "Epoch 49 \t Training Loss: 0.0012\n",
            "Final model training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the test data:\n",
        "data_test = torch.load(\"/content/drive/MyDrive/Colab Notebooks/deepfake detector/hw2_test-1.pt\")\n",
        "print(type(data_test))\n",
        "print(data_test[0].shape) # these are the ids\n",
        "print(data_test[1].shape) # these are the images\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmC8nGUgDVu9",
        "outputId": "1ae661d4-a1da-4499-8c6a-5fd1a9c25638"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "torch.Size([500])\n",
            "torch.Size([500, 3, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load and predict on Test Data\n",
        "X_test_raw= data_test[1]\n",
        "Y_test_ids = data_test[0]\n",
        "\n",
        "X_test_emb = extract_features(X_test_raw)\n",
        "final_model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = final_model(X_test_emb)\n",
        "    predictions = torch.argmax(test_outputs, dim=1).numpy()\n",
        "\n",
        "# Save to CSV file\n",
        "df = pd.DataFrame({'id': Y_test_ids, 'label': predictions})\n",
        "df.to_csv(\"predictions.csv\", index=False)"
      ],
      "metadata": {
        "id": "s1_G796PAaHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Model_3 = Model_2+ Dropout\n",
        "\n",
        "- Hyperparameter: Learning Rate=0.1, Weight Decay=0.0001\n",
        "  - dropout_prob = 0.5  ---- Test Set accuracy 0.902\n",
        "\n",
        "  - dropout_prob =0.3 --- Test Set Accuracy 0.916\n",
        "\n"
      ],
      "metadata": {
        "id": "0K2CNQA2GBHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A. MLP architecture:\n",
        "\n",
        "class MLPClassifier_dropout(nn.Module):\n",
        "    def __init__(self, input_dim=512, n_hidden=128, dropout_prob=0.3):\n",
        "\n",
        "        super(MLPClassifier_dropout, self).__init__()\n",
        "        # 2 hidden layers with 128 units, ReLU, and output dimension 2\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_dim, n_hidden),  # First hidden layer\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout_prob),       # Dropout added here\n",
        "            nn.Linear(n_hidden, n_hidden),   # Second hidden layer\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout_prob), # Dropout added here\n",
        "            nn.Linear(n_hidden, 2)           # Output logits\n",
        "                                             #dimension n_classes = 2\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)"
      ],
      "metadata": {
        "id": "Zwy6hjvCHKXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #C. Re-Training the model with Cross Entropy loss(): W/O best hyperparameter\n",
        "\n",
        "# Define the model and the optimizer and the loss criterion:\n",
        "lr = best_params[0]; wd = best_params[1]\n",
        "model = MLPClassifier_dropout()  # init model\n",
        "optimizer = SGD(model.parameters(), lr=lr, weight_decay=wd)\n",
        "loss_fn = nn.CrossEntropyLoss()  # define loss\n",
        "\n",
        "\n",
        "# Load the Tensor Dataset:\n",
        "#full_dataset = TensorDataset(X_emb, Y_raw)\n",
        "train_dataloader = DataLoader(TensorDataset(X_emb[:1500],Y_raw[:1500]), batch_size=50, shuffle=True)\n",
        "val_dataloader = DataLoader(TensorDataset(X_emb[1500:],Y_raw[1500:]), batch_size=50, shuffle=True)  # validation data!\n",
        "\n",
        "\n",
        "# Train the model:\n",
        "n_epochs=50\n",
        "for i in range(n_epochs):\n",
        "    running_loss = 0.0\n",
        "    model.train()\n",
        "    for x_batch, y_batch in train_dataloader:  # 1 epoch\n",
        "        optimizer.zero_grad()  # important to zero out the gradient buffer first\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(x_batch)  # 1st step of forward - predict something\n",
        "        loss = loss_fn(logits, y_batch)  # 2nd step - get loss, comparing prediction to ground truth\n",
        "\n",
        "        # Backward pass - gradient of loss wrt parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Critical part: update weights!\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() / len(train_dataloader)\n",
        "\n",
        "    # Calculate the loss on VALIDATION data\n",
        "    model.eval()\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    val_loss = 0.\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch in val_dataloader:\n",
        "            # Forward pass\n",
        "            logits = model(x_batch)  # 1st step of forward - predict something\n",
        "            loss = loss_fn(logits, y_batch)  # 2nd step - get loss, comparing prediction to ground truth\n",
        "            val_loss += loss.item() / len(val_dataloader)\n",
        "\n",
        "            # Accuracy Calculation:\n",
        "            # Get the predicted class (0 or 1) by finding the max logit\n",
        "            predictions = torch.argmax(logits, dim=1)\n",
        "            # Update the counters\n",
        "            correct_predictions += (predictions == y_batch).sum().item()\n",
        "            total_predictions += y_batch.size(0)\n",
        "\n",
        "    # Calculate final accuracy as a ratio\n",
        "    val_accuracy = correct_predictions / total_predictions\n",
        "    print(f\"epoch {i} \\t Training loss={running_loss:.3f} || Validation loss={val_loss:.3f}|| Validation Accuracy ={val_accuracy:0.3f}\") # Loss is decresing\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85XNmZSOIFsb",
        "outputId": "f63a63e1-a259-438e-d8a7-d511a868b856"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 \t Training loss=0.608 || Validation loss=0.471|| Validation Accuracy =0.788\n",
            "epoch 1 \t Training loss=0.450 || Validation loss=0.386|| Validation Accuracy =0.814\n",
            "epoch 2 \t Training loss=0.381 || Validation loss=0.362|| Validation Accuracy =0.840\n",
            "epoch 3 \t Training loss=0.321 || Validation loss=0.275|| Validation Accuracy =0.888\n",
            "epoch 4 \t Training loss=0.324 || Validation loss=0.255|| Validation Accuracy =0.890\n",
            "epoch 5 \t Training loss=0.286 || Validation loss=0.266|| Validation Accuracy =0.900\n",
            "epoch 6 \t Training loss=0.262 || Validation loss=0.305|| Validation Accuracy =0.854\n",
            "epoch 7 \t Training loss=0.234 || Validation loss=0.285|| Validation Accuracy =0.870\n",
            "epoch 8 \t Training loss=0.240 || Validation loss=0.294|| Validation Accuracy =0.886\n",
            "epoch 9 \t Training loss=0.230 || Validation loss=0.242|| Validation Accuracy =0.892\n",
            "epoch 10 \t Training loss=0.234 || Validation loss=0.229|| Validation Accuracy =0.900\n",
            "epoch 11 \t Training loss=0.206 || Validation loss=0.222|| Validation Accuracy =0.896\n",
            "epoch 12 \t Training loss=0.181 || Validation loss=0.230|| Validation Accuracy =0.906\n",
            "epoch 13 \t Training loss=0.217 || Validation loss=0.296|| Validation Accuracy =0.888\n",
            "epoch 14 \t Training loss=0.172 || Validation loss=0.255|| Validation Accuracy =0.902\n",
            "epoch 15 \t Training loss=0.144 || Validation loss=0.257|| Validation Accuracy =0.906\n",
            "epoch 16 \t Training loss=0.157 || Validation loss=0.254|| Validation Accuracy =0.888\n",
            "epoch 17 \t Training loss=0.156 || Validation loss=0.291|| Validation Accuracy =0.900\n",
            "epoch 18 \t Training loss=0.143 || Validation loss=0.313|| Validation Accuracy =0.888\n",
            "epoch 19 \t Training loss=0.118 || Validation loss=0.242|| Validation Accuracy =0.908\n",
            "epoch 20 \t Training loss=0.129 || Validation loss=0.250|| Validation Accuracy =0.906\n",
            "epoch 21 \t Training loss=0.135 || Validation loss=0.245|| Validation Accuracy =0.896\n",
            "epoch 22 \t Training loss=0.104 || Validation loss=0.239|| Validation Accuracy =0.906\n",
            "epoch 23 \t Training loss=0.095 || Validation loss=0.370|| Validation Accuracy =0.876\n",
            "epoch 24 \t Training loss=0.157 || Validation loss=0.503|| Validation Accuracy =0.842\n",
            "epoch 25 \t Training loss=0.195 || Validation loss=0.275|| Validation Accuracy =0.894\n",
            "epoch 26 \t Training loss=0.124 || Validation loss=0.257|| Validation Accuracy =0.910\n",
            "epoch 27 \t Training loss=0.100 || Validation loss=0.303|| Validation Accuracy =0.892\n",
            "epoch 28 \t Training loss=0.093 || Validation loss=0.247|| Validation Accuracy =0.910\n",
            "epoch 29 \t Training loss=0.080 || Validation loss=0.274|| Validation Accuracy =0.910\n",
            "epoch 30 \t Training loss=0.085 || Validation loss=0.266|| Validation Accuracy =0.906\n",
            "epoch 31 \t Training loss=0.070 || Validation loss=0.271|| Validation Accuracy =0.906\n",
            "epoch 32 \t Training loss=0.064 || Validation loss=0.251|| Validation Accuracy =0.912\n",
            "epoch 33 \t Training loss=0.098 || Validation loss=0.271|| Validation Accuracy =0.904\n",
            "epoch 34 \t Training loss=0.106 || Validation loss=0.308|| Validation Accuracy =0.910\n",
            "epoch 35 \t Training loss=0.071 || Validation loss=0.281|| Validation Accuracy =0.914\n",
            "epoch 36 \t Training loss=0.050 || Validation loss=0.297|| Validation Accuracy =0.908\n",
            "epoch 37 \t Training loss=0.051 || Validation loss=0.316|| Validation Accuracy =0.894\n",
            "epoch 38 \t Training loss=0.045 || Validation loss=0.336|| Validation Accuracy =0.908\n",
            "epoch 39 \t Training loss=0.055 || Validation loss=0.310|| Validation Accuracy =0.906\n",
            "epoch 40 \t Training loss=0.035 || Validation loss=0.388|| Validation Accuracy =0.898\n",
            "epoch 41 \t Training loss=0.027 || Validation loss=0.330|| Validation Accuracy =0.904\n",
            "epoch 42 \t Training loss=0.035 || Validation loss=0.373|| Validation Accuracy =0.906\n",
            "epoch 43 \t Training loss=0.027 || Validation loss=0.316|| Validation Accuracy =0.914\n",
            "epoch 44 \t Training loss=0.289 || Validation loss=0.247|| Validation Accuracy =0.900\n",
            "epoch 45 \t Training loss=0.105 || Validation loss=0.262|| Validation Accuracy =0.908\n",
            "epoch 46 \t Training loss=0.090 || Validation loss=0.261|| Validation Accuracy =0.908\n",
            "epoch 47 \t Training loss=0.064 || Validation loss=0.284|| Validation Accuracy =0.910\n",
            "epoch 48 \t Training loss=0.076 || Validation loss=0.251|| Validation Accuracy =0.920\n",
            "epoch 49 \t Training loss=0.064 || Validation loss=0.279|| Validation Accuracy =0.908\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final training on all data :\n",
        "full_dataset = TensorDataset(X_emb, Y_raw)\n",
        "full_loader = DataLoader(full_dataset, batch_size=50, shuffle=True)\n",
        "\n",
        "# Initialize the final model with best hyperparameters:\n",
        "final_model = MLPClassifier_dropout() # Create a new instance of the model\n",
        "loss_fn = nn.CrossEntropyLoss() # Define loss function\n",
        "final_optimizer = SGD(final_model.parameters(), lr=best_params[0], weight_decay=best_params[1])\n",
        "\n",
        "# Train the final model on the entire dataset\n",
        "n_epochs = 50\n",
        "print(f\"Training model with dropout with LR={best_params[0]} and WD={best_params[1]} for {n_epochs} epochs\")\n",
        "print(\"------------\")\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    final_model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for x_batch, y_batch in full_loader:\n",
        "        final_optimizer.zero_grad()\n",
        "        logits = final_model(x_batch)\n",
        "        loss = loss_fn(logits, y_batch)\n",
        "        loss.backward()\n",
        "        final_optimizer.step()\n",
        "        running_loss += loss.item() / len(full_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch} \\t Training Loss: {running_loss:.4f}\")\n",
        "\n",
        "print(\"Final model training complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXqtRNVPIvzb",
        "outputId": "2ae9b6d6-4db8-4f97-9abf-01123e36b1ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model with dropout with LR=0.1 and WD=0.0001 for 50 epochs\n",
            "------------\n",
            "Epoch 0 \t Training Loss: 0.5568\n",
            "Epoch 1 \t Training Loss: 0.3964\n",
            "Epoch 2 \t Training Loss: 0.3269\n",
            "Epoch 3 \t Training Loss: 0.2937\n",
            "Epoch 4 \t Training Loss: 0.2710\n",
            "Epoch 5 \t Training Loss: 0.2439\n",
            "Epoch 6 \t Training Loss: 0.2377\n",
            "Epoch 7 \t Training Loss: 0.2334\n",
            "Epoch 8 \t Training Loss: 0.2037\n",
            "Epoch 9 \t Training Loss: 0.1957\n",
            "Epoch 10 \t Training Loss: 0.1877\n",
            "Epoch 11 \t Training Loss: 0.1794\n",
            "Epoch 12 \t Training Loss: 0.1866\n",
            "Epoch 13 \t Training Loss: 0.1533\n",
            "Epoch 14 \t Training Loss: 0.1598\n",
            "Epoch 15 \t Training Loss: 0.1359\n",
            "Epoch 16 \t Training Loss: 0.1558\n",
            "Epoch 17 \t Training Loss: 0.1437\n",
            "Epoch 18 \t Training Loss: 0.1368\n",
            "Epoch 19 \t Training Loss: 0.1482\n",
            "Epoch 20 \t Training Loss: 0.1228\n",
            "Epoch 21 \t Training Loss: 0.1191\n",
            "Epoch 22 \t Training Loss: 0.0993\n",
            "Epoch 23 \t Training Loss: 0.0911\n",
            "Epoch 24 \t Training Loss: 0.0884\n",
            "Epoch 25 \t Training Loss: 0.1462\n",
            "Epoch 26 \t Training Loss: 0.0791\n",
            "Epoch 27 \t Training Loss: 0.0909\n",
            "Epoch 28 \t Training Loss: 0.0748\n",
            "Epoch 29 \t Training Loss: 0.0795\n",
            "Epoch 30 \t Training Loss: 0.0647\n",
            "Epoch 31 \t Training Loss: 0.0567\n",
            "Epoch 32 \t Training Loss: 0.0495\n",
            "Epoch 33 \t Training Loss: 0.1065\n",
            "Epoch 34 \t Training Loss: 0.0984\n",
            "Epoch 35 \t Training Loss: 0.0484\n",
            "Epoch 36 \t Training Loss: 0.0496\n",
            "Epoch 37 \t Training Loss: 0.1084\n",
            "Epoch 38 \t Training Loss: 0.0666\n",
            "Epoch 39 \t Training Loss: 0.0625\n",
            "Epoch 40 \t Training Loss: 0.0447\n",
            "Epoch 41 \t Training Loss: 0.0568\n",
            "Epoch 42 \t Training Loss: 0.0327\n",
            "Epoch 43 \t Training Loss: 0.0374\n",
            "Epoch 44 \t Training Loss: 0.0334\n",
            "Epoch 45 \t Training Loss: 0.0262\n",
            "Epoch 46 \t Training Loss: 0.0252\n",
            "Epoch 47 \t Training Loss: 0.0244\n",
            "Epoch 48 \t Training Loss: 0.0228\n",
            "Epoch 49 \t Training Loss: 0.0338\n",
            "Final model training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load and predict on Test Data\n",
        "data_test = torch.load(\"/content/drive/MyDrive/Colab Notebooks/deepfake detector/hw2_test-1.pt\")\n",
        "X_test_raw= data_test[1]\n",
        "Y_test_ids = data_test[0]\n",
        "\n",
        "X_test_emb = extract_features(X_test_raw)\n",
        "final_model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = final_model(X_test_emb)\n",
        "    predictions = torch.argmax(test_outputs, dim=1).numpy()\n",
        "\n",
        "# Save to CSV file\n",
        "df = pd.DataFrame({'id': Y_test_ids, 'label': predictions})\n",
        "df.to_csv(\"predictions.csv\", index=False)"
      ],
      "metadata": {
        "id": "4Gb7itbnJqGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fjjMio4HL74i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Model_4 = Baseline + Dropout + HP tuning\n",
        "\n",
        "- Dropout =0.3\n",
        "  - Best Parameter : Learning Rate=0.01, Weight Decay=0 with Validation Accuracy=0.9160\n",
        "  - Test Set Accuracy 0.908\n",
        "\n",
        "- Dropout =0.5\n",
        "  - Best Parameters: Learning Rate=0.05, Weight Decay=0.0001 with Validation Accuracy=0.9205\n",
        "  - Test Set Accuracy 0.9000\n",
        "\n"
      ],
      "metadata": {
        "id": "UegrY5w8L8TF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #A. MLP architecture:\n",
        "\n",
        "class MLPClassifier_dropout(nn.Module):\n",
        "    def __init__(self, input_dim=512, n_hidden=128, dropout_prob=0.5):\n",
        "\n",
        "        super(MLPClassifier_dropout, self).__init__()\n",
        "        # 2 hidden layers with 128 units, ReLU, and output dimension 2\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_dim, n_hidden),  # First hidden layer\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout_prob),       # Dropout added here\n",
        "            nn.Linear(n_hidden, n_hidden),   # Second hidden layer\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout_prob), # Dropout added here\n",
        "            nn.Linear(n_hidden, 2)           # Output logits\n",
        "                                             #dimension n_classes = 2\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)"
      ],
      "metadata": {
        "id": "aPsQjNnqO31P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# D. Model with Hyper parameter tuning with K-fold:\n",
        "\n",
        "# Hyperparameter Grid:\n",
        "learning_rates = [0.001, 0.01, 0.05, 0.1]\n",
        "weight_decays = [0, 1e-4, 1e-3, 1e-2]\n",
        "print(\"Hyper parameter Grid\")\n",
        "print(f\"Learning Rates: {learning_rates}\")\n",
        "print(f\"Weight Decay: {weight_decays}\")\n",
        "\n",
        "n_epochs=50\n",
        "\n",
        "# Prepare K-Fold:\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42) # 5-fold CV with shuffling\n",
        "full_dataset = TensorDataset(X_emb, Y_raw)\n",
        "\n",
        "# counters for best parameter update:\n",
        "best_acc = 0\n",
        "best_params = (None, None)\n",
        "\n",
        "print(\"--------------------\")\n",
        "print(\"Starting Grid Search CV...\")\n",
        "for lr in learning_rates:\n",
        "    for wd in weight_decays:\n",
        "\n",
        "        #print(\"---------------------\")\n",
        "        #print(f\"\\nLearning Rate: {lr}, Weight Decay: {wd}\")\n",
        "\n",
        "        fold_accuracies = [] # List of fold accuracies\n",
        "        train_loss=[]\n",
        "        validation_loss=[]\n",
        "        #fold=0\n",
        "        for train_idx, val_idx in kf.split(full_dataset):\n",
        "            #print(f\" \\nFold {fold}\")\n",
        "            #fold+=1\n",
        "\n",
        "            # Split data\n",
        "            train_sub = Subset(full_dataset, train_idx)\n",
        "            val_sub = Subset(full_dataset, val_idx)\n",
        "\n",
        "            train_loader = DataLoader(train_sub, batch_size=50, shuffle=True)\n",
        "            val_loader = DataLoader(val_sub, batch_size=50, shuffle=True)\n",
        "\n",
        "            # Initialize Model, Loss, Optimizer\n",
        "            model = MLPClassifier_dropout()\n",
        "            loss_fn = nn.CrossEntropyLoss() # Use Cross Entropy loss\n",
        "            optimizer =SGD(model.parameters(), lr=lr, weight_decay=wd) # SGD optimizer\n",
        "\n",
        "            # Train for n_epochs\n",
        "            model.train()\n",
        "\n",
        "            for epoch in range(n_epochs):\n",
        "                running_loss = 0.0\n",
        "                for batch_X, batch_y in train_loader:\n",
        "                    optimizer.zero_grad()\n",
        "                    logits = model(batch_X)\n",
        "                    loss = loss_fn(logits, batch_y)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    running_loss += loss.item() / len(train_loader)\n",
        "            #print(f\"training loss = {running_loss : 0.4f}\")\n",
        "            train_loss.append(running_loss)\n",
        "            # Validate\n",
        "            model.eval()\n",
        "            correct_predictions=0\n",
        "            total_predictions = 0\n",
        "            val_loss=0.0\n",
        "            with torch.no_grad():\n",
        "                for batch_X, batch_y in val_loader:\n",
        "                    logits = model(batch_X)\n",
        "                    loss = loss_fn(logits, batch_y)\n",
        "                    val_loss += loss.item() / len(val_loader)\n",
        "                    predictions = torch.argmax(logits, dim=1)\n",
        "                    correct_predictions += (predictions == batch_y).sum().item()\n",
        "                    total_predictions += batch_y.size(0)\n",
        "                val_acc= correct_predictions/total_predictions\n",
        "            #print(f\"validation loss = {val_loss : 0.4f}\")\n",
        "            #print(f\"validation accuracy = {val_acc : 0.4f}\")\n",
        "\n",
        "            validation_loss.append(val_loss)\n",
        "            #fold_accuracies.append(correct_predictions / total_predictions)\n",
        "            fold_accuracies.append(val_acc)\n",
        "\n",
        "        mean_val_acc = np.mean(fold_accuracies) # Choose hyperparameters by mean validation accuracy\n",
        "        mean_train_loss=np.mean(train_loss)\n",
        "        mean_val_loss=np.mean(validation_loss)\n",
        "\n",
        "        print(\"-----------\")\n",
        "        print(f\"Learning Rate: {lr}, Weight Decay: {wd}\")\n",
        "        print(f\"Train_Loss ={mean_train_loss:0.4f}|| Validation_loss ={mean_val_loss:0.4f}|| Validation_accurancy ={mean_val_acc:.4f},\")\n",
        "\n",
        "        if mean_val_acc > best_acc:\n",
        "            best_acc = mean_val_acc\n",
        "            best_params = (lr, wd)\n",
        "print(\"\\n------------------------------\")\n",
        "print(f\"Best Parameters: Learning Rate={best_params[0]}, Weight Decay={best_params[1]} with Validation Accuracy={best_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyIvdfRbMPEw",
        "outputId": "a71d5d3b-7f8b-4778-d525-e3e13acab9d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyper parameter Grid\n",
            "Learning Rates: [0.001, 0.01, 0.05, 0.1]\n",
            "Weight Decay: [0, 0.0001, 0.001, 0.01]\n",
            "--------------------\n",
            "Starting Grid Search CV...\n",
            "-----------\n",
            "Learning Rate: 0.001, Weight Decay: 0\n",
            "Train_Loss =0.6031|| Validation_loss =0.5858|| Validation_accurancy =0.7875,\n",
            "-----------\n",
            "Learning Rate: 0.001, Weight Decay: 0.0001\n",
            "Train_Loss =0.6061|| Validation_loss =0.5848|| Validation_accurancy =0.8070,\n",
            "-----------\n",
            "Learning Rate: 0.001, Weight Decay: 0.001\n",
            "Train_Loss =0.5961|| Validation_loss =0.5752|| Validation_accurancy =0.8020,\n",
            "-----------\n",
            "Learning Rate: 0.001, Weight Decay: 0.01\n",
            "Train_Loss =0.6148|| Validation_loss =0.6025|| Validation_accurancy =0.7760,\n",
            "-----------\n",
            "Learning Rate: 0.01, Weight Decay: 0\n",
            "Train_Loss =0.1965|| Validation_loss =0.2299|| Validation_accurancy =0.9080,\n",
            "-----------\n",
            "Learning Rate: 0.01, Weight Decay: 0.0001\n",
            "Train_Loss =0.1943|| Validation_loss =0.2332|| Validation_accurancy =0.9110,\n",
            "-----------\n",
            "Learning Rate: 0.01, Weight Decay: 0.001\n",
            "Train_Loss =0.2051|| Validation_loss =0.2285|| Validation_accurancy =0.9125,\n",
            "-----------\n",
            "Learning Rate: 0.01, Weight Decay: 0.01\n",
            "Train_Loss =0.2004|| Validation_loss =0.2368|| Validation_accurancy =0.9065,\n",
            "-----------\n",
            "Learning Rate: 0.05, Weight Decay: 0\n",
            "Train_Loss =0.0730|| Validation_loss =0.2957|| Validation_accurancy =0.9095,\n",
            "-----------\n",
            "Learning Rate: 0.05, Weight Decay: 0.0001\n",
            "Train_Loss =0.0697|| Validation_loss =0.2650|| Validation_accurancy =0.9205,\n",
            "-----------\n",
            "Learning Rate: 0.05, Weight Decay: 0.001\n",
            "Train_Loss =0.0681|| Validation_loss =0.2673|| Validation_accurancy =0.9155,\n",
            "-----------\n",
            "Learning Rate: 0.05, Weight Decay: 0.01\n",
            "Train_Loss =0.1306|| Validation_loss =0.2481|| Validation_accurancy =0.9090,\n",
            "-----------\n",
            "Learning Rate: 0.1, Weight Decay: 0\n",
            "Train_Loss =0.0661|| Validation_loss =0.3097|| Validation_accurancy =0.9100,\n",
            "-----------\n",
            "Learning Rate: 0.1, Weight Decay: 0.0001\n",
            "Train_Loss =0.0817|| Validation_loss =0.2809|| Validation_accurancy =0.9135,\n",
            "-----------\n",
            "Learning Rate: 0.1, Weight Decay: 0.001\n",
            "Train_Loss =0.0877|| Validation_loss =0.2692|| Validation_accurancy =0.9175,\n",
            "-----------\n",
            "Learning Rate: 0.1, Weight Decay: 0.01\n",
            "Train_Loss =0.1576|| Validation_loss =0.2207|| Validation_accurancy =0.9130,\n",
            "\n",
            "------------------------------\n",
            "Best Parameters: Learning Rate=0.05, Weight Decay=0.0001 with Validation Accuracy=0.9205\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final training on all data :\n",
        "full_dataset = TensorDataset(X_emb, Y_raw)\n",
        "full_loader = DataLoader(full_dataset, batch_size=50, shuffle=True)\n",
        "\n",
        "# Initialize the final model with best hyperparameters:\n",
        "final_model = MLPClassifier_dropout() # Create a new instance of the model\n",
        "loss_fn = nn.CrossEntropyLoss() # Define loss function\n",
        "final_optimizer = SGD(final_model.parameters(), lr=best_params[0], weight_decay=best_params[1])\n",
        "\n",
        "# Train the final model on the entire dataset\n",
        "n_epochs = 50\n",
        "print(f\"Training final model with LR={best_params[0]} and WD={best_params[1]} for {n_epochs} epochs\")\n",
        "print(\"------------\")\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    final_model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for x_batch, y_batch in full_loader:\n",
        "        final_optimizer.zero_grad()\n",
        "        logits = final_model(x_batch)\n",
        "        loss = loss_fn(logits, y_batch)\n",
        "        loss.backward()\n",
        "        final_optimizer.step()\n",
        "        running_loss += loss.item() / len(full_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch} \\t Training Loss: {running_loss:.4f}\")\n",
        "\n",
        "print(\"Final model training complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ct_rcbON6qK",
        "outputId": "80776c3d-b1d7-4b17-c871-0cbd537a553c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training final model with LR=0.05 and WD=0.0001 for 50 epochs\n",
            "------------\n",
            "Epoch 0 \t Training Loss: 0.6467\n",
            "Epoch 1 \t Training Loss: 0.4952\n",
            "Epoch 2 \t Training Loss: 0.3916\n",
            "Epoch 3 \t Training Loss: 0.3394\n",
            "Epoch 4 \t Training Loss: 0.3250\n",
            "Epoch 5 \t Training Loss: 0.2822\n",
            "Epoch 6 \t Training Loss: 0.2798\n",
            "Epoch 7 \t Training Loss: 0.2537\n",
            "Epoch 8 \t Training Loss: 0.2385\n",
            "Epoch 9 \t Training Loss: 0.2525\n",
            "Epoch 10 \t Training Loss: 0.2400\n",
            "Epoch 11 \t Training Loss: 0.2138\n",
            "Epoch 12 \t Training Loss: 0.2066\n",
            "Epoch 13 \t Training Loss: 0.2095\n",
            "Epoch 14 \t Training Loss: 0.1968\n",
            "Epoch 15 \t Training Loss: 0.1927\n",
            "Epoch 16 \t Training Loss: 0.1852\n",
            "Epoch 17 \t Training Loss: 0.1851\n",
            "Epoch 18 \t Training Loss: 0.1741\n",
            "Epoch 19 \t Training Loss: 0.1652\n",
            "Epoch 20 \t Training Loss: 0.1513\n",
            "Epoch 21 \t Training Loss: 0.1591\n",
            "Epoch 22 \t Training Loss: 0.1676\n",
            "Epoch 23 \t Training Loss: 0.1433\n",
            "Epoch 24 \t Training Loss: 0.1471\n",
            "Epoch 25 \t Training Loss: 0.1469\n",
            "Epoch 26 \t Training Loss: 0.1277\n",
            "Epoch 27 \t Training Loss: 0.1417\n",
            "Epoch 28 \t Training Loss: 0.1192\n",
            "Epoch 29 \t Training Loss: 0.1226\n",
            "Epoch 30 \t Training Loss: 0.1111\n",
            "Epoch 31 \t Training Loss: 0.1045\n",
            "Epoch 32 \t Training Loss: 0.1235\n",
            "Epoch 33 \t Training Loss: 0.1094\n",
            "Epoch 34 \t Training Loss: 0.1093\n",
            "Epoch 35 \t Training Loss: 0.1113\n",
            "Epoch 36 \t Training Loss: 0.1077\n",
            "Epoch 37 \t Training Loss: 0.0978\n",
            "Epoch 38 \t Training Loss: 0.0962\n",
            "Epoch 39 \t Training Loss: 0.0903\n",
            "Epoch 40 \t Training Loss: 0.0859\n",
            "Epoch 41 \t Training Loss: 0.1036\n",
            "Epoch 42 \t Training Loss: 0.1056\n",
            "Epoch 43 \t Training Loss: 0.0947\n",
            "Epoch 44 \t Training Loss: 0.0844\n",
            "Epoch 45 \t Training Loss: 0.0777\n",
            "Epoch 46 \t Training Loss: 0.0732\n",
            "Epoch 47 \t Training Loss: 0.0763\n",
            "Epoch 48 \t Training Loss: 0.0799\n",
            "Epoch 49 \t Training Loss: 0.0863\n",
            "Final model training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load and predict on Test Data\n",
        "data_test = torch.load(\"/content/drive/MyDrive/Colab Notebooks/deepfake detector/hw2_test-1.pt\")\n",
        "X_test_raw= data_test[1]\n",
        "Y_test_ids = data_test[0]\n",
        "\n",
        "X_test_emb = extract_features(X_test_raw)\n",
        "final_model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = final_model(X_test_emb)\n",
        "    predictions = torch.argmax(test_outputs, dim=1).numpy()\n",
        "\n",
        "# Save to CSV file\n",
        "df = pd.DataFrame({'id': Y_test_ids, 'label': predictions})\n",
        "df.to_csv(\"predictions.csv\", index=False)"
      ],
      "metadata": {
        "id": "XshPbZfkOV_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Model_5 = Model_2 with Adam Optimizer\n",
        "\n",
        "- Best Parameters: Learning Rate=0.001, Weight Decay=0 with Validation Accuracy=0.9055\n",
        "\n",
        "- Test Data Accuracy 0.9140\n"
      ],
      "metadata": {
        "id": "194IaPHnRn24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# B. MLP architecture:\n",
        "\n",
        "class MLPClassifier(nn.Module):\n",
        "    def __init__(self, input_dim=512, n_hidden=128):\n",
        "        super(MLPClassifier, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_dim, n_hidden),  # First hidden layer\n",
        "            nn.ReLU(),                       # ReLU nonlinearity\n",
        "            nn.Linear(n_hidden, n_hidden),   # Second hidden layer\n",
        "            nn.ReLU(),                       # ReLU nonlinearity\n",
        "            nn.Linear(n_hidden, 2)           # Output logits\n",
        "                                             #dimension n_classes = 2\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "\n",
        "# D. Model with Hyper parameter tuning with K-fold:\n",
        "\n",
        "# Hyperparameter Grid:\n",
        "learning_rates = [0.001, 0.01, 0.05, 0.1]\n",
        "weight_decays = [0, 1e-4, 1e-3, 1e-2]\n",
        "print(\"Hyper parameter Grid\")\n",
        "print(f\"Learning Rates: {learning_rates}\")\n",
        "print(f\"Weight Decay: {weight_decays}\")\n",
        "\n",
        "n_epochs=50\n",
        "\n",
        "# Prepare K-Fold:\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42) # 5-fold CV with shuffling\n",
        "full_dataset = TensorDataset(X_emb, Y_raw)\n",
        "\n",
        "# counters for best parameter update:\n",
        "best_acc = 0\n",
        "best_params = (None, None)\n",
        "\n",
        "print(\"--------------------\")\n",
        "print(\"Starting Grid Search CV...\")\n",
        "for lr in learning_rates:\n",
        "    for wd in weight_decays:\n",
        "\n",
        "        #print(\"---------------------\")\n",
        "        #print(f\"\\nLearning Rate: {lr}, Weight Decay: {wd}\")\n",
        "\n",
        "        fold_accuracies = [] # List of fold accuracies\n",
        "        train_loss=[]\n",
        "        validation_loss=[]\n",
        "        #fold=0\n",
        "        for train_idx, val_idx in kf.split(full_dataset):\n",
        "            #print(f\" \\nFold {fold}\")\n",
        "            #fold+=1\n",
        "\n",
        "            # Split data\n",
        "            train_sub = Subset(full_dataset, train_idx)\n",
        "            val_sub = Subset(full_dataset, val_idx)\n",
        "\n",
        "            train_loader = DataLoader(train_sub, batch_size=50, shuffle=True)\n",
        "            val_loader = DataLoader(val_sub, batch_size=50, shuffle=True)\n",
        "\n",
        "            # Initialize Model, Loss, Optimizer\n",
        "            model = MLPClassifier()\n",
        "            loss_fn = nn.CrossEntropyLoss() # Use Cross Entropy loss\n",
        "            optimizer =Adam(model.parameters(), lr=lr, weight_decay=wd) # Adam optimizer\n",
        "\n",
        "            # Train for n_epochs\n",
        "            model.train()\n",
        "\n",
        "            for epoch in range(n_epochs):\n",
        "                running_loss = 0.0\n",
        "                for batch_X, batch_y in train_loader:\n",
        "                    optimizer.zero_grad()\n",
        "                    logits = model(batch_X)\n",
        "                    loss = loss_fn(logits, batch_y)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    running_loss += loss.item() / len(train_loader)\n",
        "            #print(f\"training loss = {running_loss : 0.4f}\")\n",
        "            train_loss.append(running_loss)\n",
        "            # Validate\n",
        "            model.eval()\n",
        "            correct_predictions=0\n",
        "            total_predictions = 0\n",
        "            val_loss=0.0\n",
        "            with torch.no_grad():\n",
        "                for batch_X, batch_y in val_loader:\n",
        "                    logits = model(batch_X)\n",
        "                    loss = loss_fn(logits, batch_y)\n",
        "                    val_loss += loss.item() / len(val_loader)\n",
        "                    predictions = torch.argmax(logits, dim=1)\n",
        "                    correct_predictions += (predictions == batch_y).sum().item()\n",
        "                    total_predictions += batch_y.size(0)\n",
        "                val_acc= correct_predictions/total_predictions\n",
        "            #print(f\"validation loss = {val_loss : 0.4f}\")\n",
        "            #print(f\"validation accuracy = {val_acc : 0.4f}\")\n",
        "\n",
        "            validation_loss.append(val_loss)\n",
        "            #fold_accuracies.append(correct_predictions / total_predictions)\n",
        "            fold_accuracies.append(val_acc)\n",
        "\n",
        "        mean_val_acc = np.mean(fold_accuracies) # Choose hyperparameters by mean validation accuracy\n",
        "        mean_train_loss=np.mean(train_loss)\n",
        "        mean_val_loss=np.mean(validation_loss)\n",
        "\n",
        "        print(\"-----------\")\n",
        "        print(f\"Learning Rate: {lr}, Weight Decay: {wd}\")\n",
        "        print(f\"Train_Loss ={mean_train_loss:0.4f}|| Validation_loss ={mean_val_loss:0.4f}|| Validation_accurancy ={mean_val_acc:.4f},\")\n",
        "\n",
        "        if mean_val_acc > best_acc:\n",
        "            best_acc = mean_val_acc\n",
        "            best_params = (lr, wd)\n",
        "print(\"\\n------------------------------\")\n",
        "print(f\"Best Parameters: Learning Rate={best_params[0]}, Weight Decay={best_params[1]} with Validation Accuracy={best_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evplHl4RRzOe",
        "outputId": "84f5546d-1d21-4e23-c9da-1ef58b029dcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyper parameter Grid\n",
            "Learning Rates: [0.001, 0.01, 0.05, 0.1]\n",
            "Weight Decay: [0, 0.0001, 0.001, 0.01]\n",
            "--------------------\n",
            "Starting Grid Search CV...\n",
            "-----------\n",
            "Learning Rate: 0.001, Weight Decay: 0\n",
            "Train_Loss =0.0000|| Validation_loss =0.6622|| Validation_accurancy =0.9055,\n",
            "-----------\n",
            "Learning Rate: 0.001, Weight Decay: 0.0001\n",
            "Train_Loss =0.0002|| Validation_loss =0.5577|| Validation_accurancy =0.9040,\n",
            "-----------\n",
            "Learning Rate: 0.001, Weight Decay: 0.001\n",
            "Train_Loss =0.0309|| Validation_loss =0.3796|| Validation_accurancy =0.9040,\n",
            "-----------\n",
            "Learning Rate: 0.001, Weight Decay: 0.01\n",
            "Train_Loss =0.0345|| Validation_loss =0.2921|| Validation_accurancy =0.9000,\n",
            "-----------\n",
            "Learning Rate: 0.01, Weight Decay: 0\n",
            "Train_Loss =0.0001|| Validation_loss =0.9683|| Validation_accurancy =0.9045,\n",
            "-----------\n",
            "Learning Rate: 0.01, Weight Decay: 0.0001\n",
            "Train_Loss =0.0102|| Validation_loss =0.8103|| Validation_accurancy =0.8990,\n",
            "-----------\n",
            "Learning Rate: 0.01, Weight Decay: 0.001\n",
            "Train_Loss =0.0371|| Validation_loss =0.4742|| Validation_accurancy =0.8925,\n",
            "-----------\n",
            "Learning Rate: 0.01, Weight Decay: 0.01\n",
            "Train_Loss =0.1274|| Validation_loss =0.2904|| Validation_accurancy =0.8945,\n",
            "-----------\n",
            "Learning Rate: 0.05, Weight Decay: 0\n",
            "Train_Loss =0.3009|| Validation_loss =0.6963|| Validation_accurancy =0.7365,\n",
            "-----------\n",
            "Learning Rate: 0.05, Weight Decay: 0.0001\n",
            "Train_Loss =0.0937|| Validation_loss =0.4698|| Validation_accurancy =0.8830,\n",
            "-----------\n",
            "Learning Rate: 0.05, Weight Decay: 0.001\n",
            "Train_Loss =0.4460|| Validation_loss =0.4214|| Validation_accurancy =0.7995,\n",
            "-----------\n",
            "Learning Rate: 0.05, Weight Decay: 0.01\n",
            "Train_Loss =0.4039|| Validation_loss =0.3359|| Validation_accurancy =0.8660,\n",
            "-----------\n",
            "Learning Rate: 0.1, Weight Decay: 0\n",
            "Train_Loss =0.3290|| Validation_loss =0.5692|| Validation_accurancy =0.7330,\n",
            "-----------\n",
            "Learning Rate: 0.1, Weight Decay: 0.0001\n",
            "Train_Loss =0.6363|| Validation_loss =0.6559|| Validation_accurancy =0.5425,\n",
            "-----------\n",
            "Learning Rate: 0.1, Weight Decay: 0.001\n",
            "Train_Loss =0.7759|| Validation_loss =3.4566|| Validation_accurancy =0.4850,\n",
            "-----------\n",
            "Learning Rate: 0.1, Weight Decay: 0.01\n",
            "Train_Loss =1.5028|| Validation_loss =1.2937|| Validation_accurancy =0.5435,\n",
            "\n",
            "------------------------------\n",
            "Best Parameters: Learning Rate=0.001, Weight Decay=0 with Validation Accuracy=0.9055\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AFvCY57S5To",
        "outputId": "f5cda85c-09d2-41d3-802a-4a63b37e3947"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.001, 0)"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final training on all data :\n",
        "full_dataset = TensorDataset(X_emb, Y_raw)\n",
        "full_loader = DataLoader(full_dataset, batch_size=50, shuffle=True)\n",
        "\n",
        "# Initialize the final model with best hyperparameters:\n",
        "final_model = MLPClassifier() # Create a new instance of the model\n",
        "loss_fn = nn.CrossEntropyLoss() # Define loss function\n",
        "final_optimizer = Adam(final_model.parameters(), lr=best_params[0], weight_decay=best_params[1])\n",
        "\n",
        "# Train the final model on the entire dataset\n",
        "n_epochs = 50\n",
        "print(f\"Training final model with LR={best_params[0]} and WD={best_params[1]} for {n_epochs} epochs\")\n",
        "print(\"------------\")\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    final_model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for x_batch, y_batch in full_loader:\n",
        "        final_optimizer.zero_grad()\n",
        "        logits = final_model(x_batch)\n",
        "        loss = loss_fn(logits, y_batch)\n",
        "        loss.backward()\n",
        "        final_optimizer.step()\n",
        "        running_loss += loss.item() / len(full_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch} \\t Training Loss: {running_loss:.4f}\")\n",
        "\n",
        "print(\"Final model training complete.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cvj_ngbtStxr",
        "outputId": "ff448541-d9eb-412b-9a29-b8b31ab6e3e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training final model with LR=0.001 and WD=0 for 50 epochs\n",
            "------------\n",
            "Epoch 0 \t Training Loss: 0.4495\n",
            "Epoch 1 \t Training Loss: 0.2497\n",
            "Epoch 2 \t Training Loss: 0.2236\n",
            "Epoch 3 \t Training Loss: 0.1730\n",
            "Epoch 4 \t Training Loss: 0.1583\n",
            "Epoch 5 \t Training Loss: 0.1571\n",
            "Epoch 6 \t Training Loss: 0.1253\n",
            "Epoch 7 \t Training Loss: 0.0954\n",
            "Epoch 8 \t Training Loss: 0.0915\n",
            "Epoch 9 \t Training Loss: 0.0640\n",
            "Epoch 10 \t Training Loss: 0.0641\n",
            "Epoch 11 \t Training Loss: 0.0490\n",
            "Epoch 12 \t Training Loss: 0.0403\n",
            "Epoch 13 \t Training Loss: 0.0295\n",
            "Epoch 14 \t Training Loss: 0.0268\n",
            "Epoch 15 \t Training Loss: 0.0130\n",
            "Epoch 16 \t Training Loss: 0.0086\n",
            "Epoch 17 \t Training Loss: 0.0046\n",
            "Epoch 18 \t Training Loss: 0.0018\n",
            "Epoch 19 \t Training Loss: 0.0015\n",
            "Epoch 20 \t Training Loss: 0.0010\n",
            "Epoch 21 \t Training Loss: 0.0008\n",
            "Epoch 22 \t Training Loss: 0.0007\n",
            "Epoch 23 \t Training Loss: 0.0006\n",
            "Epoch 24 \t Training Loss: 0.0005\n",
            "Epoch 25 \t Training Loss: 0.0004\n",
            "Epoch 26 \t Training Loss: 0.0003\n",
            "Epoch 27 \t Training Loss: 0.0002\n",
            "Epoch 28 \t Training Loss: 0.0002\n",
            "Epoch 29 \t Training Loss: 0.0001\n",
            "Epoch 30 \t Training Loss: 0.0001\n",
            "Epoch 31 \t Training Loss: 0.0001\n",
            "Epoch 32 \t Training Loss: 0.0001\n",
            "Epoch 33 \t Training Loss: 0.0001\n",
            "Epoch 34 \t Training Loss: 0.0001\n",
            "Epoch 35 \t Training Loss: 0.0000\n",
            "Epoch 36 \t Training Loss: 0.0000\n",
            "Epoch 37 \t Training Loss: 0.0000\n",
            "Epoch 38 \t Training Loss: 0.0000\n",
            "Epoch 39 \t Training Loss: 0.0000\n",
            "Epoch 40 \t Training Loss: 0.0000\n",
            "Epoch 41 \t Training Loss: 0.0000\n",
            "Epoch 42 \t Training Loss: 0.0000\n",
            "Epoch 43 \t Training Loss: 0.0000\n",
            "Epoch 44 \t Training Loss: 0.0000\n",
            "Epoch 45 \t Training Loss: 0.0000\n",
            "Epoch 46 \t Training Loss: 0.0000\n",
            "Epoch 47 \t Training Loss: 0.0000\n",
            "Epoch 48 \t Training Loss: 0.0000\n",
            "Epoch 49 \t Training Loss: 0.0000\n",
            "Final model training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load and predict on Test Data\n",
        "data_test = torch.load(\"/content/drive/MyDrive/Colab Notebooks/deepfake detector/hw2_test-1.pt\")\n",
        "X_test_raw= data_test[1]\n",
        "Y_test_ids = data_test[0]\n",
        "\n",
        "X_test_emb = extract_features(X_test_raw)\n",
        "final_model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = final_model(X_test_emb)\n",
        "    predictions = torch.argmax(test_outputs, dim=1).numpy()\n",
        "\n",
        "# Save to CSV file\n",
        "df = pd.DataFrame({'id': Y_test_ids, 'label': predictions})\n",
        "df.to_csv(\"predictions.csv\", index=False)"
      ],
      "metadata": {
        "id": "nTf3oYq4S0Dw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Model_6 = Simple Convolutional NN\n",
        "\n",
        " 1. Hyperparamter tuning with Adam Optimizer and Dropout_prob =0.5\n",
        "    - Trained for 50 epochs\n",
        "    - Best Parameters: Learning Rate=0.001, Weight Decay=0.0001 with Validation Accuracy=0.9500\n",
        "    - Test set accuracy 0.944\n",
        "\n",
        " 2. Hyperparameter tuning with SGD Optimizer and Dropout_prob =0.5\n",
        "    - Trained for 200 epochs  \n",
        "    - Best Parameters: Learning Rate=0.01, Weight Decay=0.01 with Validation Accuracy=0.7050\n",
        "    - Test set accuracy 0.9420  \n",
        "\n"
      ],
      "metadata": {
        "id": "KahlK2RCUbF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleConvNet, self).__init__()\n",
        "\n",
        "        # 1st Convolutional Block:\n",
        "        # Input: 3 color channels (RGB). Output: 16 feature maps.\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) # Shrinks 32x32 to 16x16\n",
        "\n",
        "        # 2nd Convolutional Block:\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) # Shrinks 16x16 to 8x8\n",
        "\n",
        "        # 3rd Convolutional Block:\n",
        "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2) # Shrinks 8x8 to 4x4\n",
        "\n",
        "        # Fully Connected (MLP) Classifier at the end:\n",
        "        # The image is now 64 channels, each 4x4 pixels.\n",
        "        # Flattened size = 64 * 4 * 4 = 1024\n",
        "        self.fc1 = nn.Linear(64 * 4 * 4, 128)\n",
        "        self.dropout = nn.Dropout(0.5) # Prevent overfitting\n",
        "        self.fc2 = nn.Linear(128, 2)   # 2 Output classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass through Conv layers with ReLU activations\n",
        "        x = self.pool1(F.relu(self.conv1(x)))\n",
        "        x = self.pool2(F.relu(self.conv2(x)))\n",
        "        x = self.pool3(F.relu(self.conv3(x)))\n",
        "\n",
        "        # Flatten the 2D maps into a 1D vector for the linear layers\n",
        "        x = x.view(-1, 64 * 4 * 4)\n",
        "\n",
        "        # Pass through the linear classifier\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "AxeMYbS0Uajw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load Data (Skiping ResNet extraction completely)\n",
        "print(\"Loading dataset\")\n",
        "X_raw, Y_raw = torch.load(\"/content/drive/MyDrive/Colab Notebooks/deepfake detector/hw2_data.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdjt5z1iUu8X",
        "outputId": "1a9d29b9-ac00-4692-87ba-f9554fab44dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Full Data set :\n",
        "full_dataset = TensorDataset(X_raw, Y_raw)"
      ],
      "metadata": {
        "id": "B5d4A5U-Vq0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Model with Hyper parameter tuning with K-fold:\n",
        "\n",
        "# Hyperparameter Grid:\n",
        "learning_rates = [0.0001, 0.001, 0.01]\n",
        "weight_decays = [0, 1e-4, 1e-3, 1e-2]\n",
        "print(\"Hyper parameter Grid\")\n",
        "print(f\"Learning Rates: {learning_rates}\")\n",
        "print(f\"Weight Decay: {weight_decays}\")\n",
        "\n",
        "n_epochs=50\n",
        "\n",
        "# Prepare K-Fold:\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42) # 5-fold CV with shuffling\n",
        "full_dataset = TensorDataset(X_raw, Y_raw) # Changed from X_emb to X_raw\n",
        "\n",
        "# counters for best parameter update:\n",
        "best_acc = 0\n",
        "best_params = (None, None)\n",
        "\n",
        "print(\"--------------------\")\n",
        "print(\"Starting Grid Search CV...\")\n",
        "for lr in learning_rates:\n",
        "    for wd in weight_decays:\n",
        "\n",
        "        #print(\"---------------------\")\n",
        "        #print(f\"\\nLearning Rate: {lr}, Weight Decay: {wd}\")\n",
        "\n",
        "        fold_accuracies = [] # List of fold accuracies\n",
        "        train_loss=[]\n",
        "        validation_loss=[]\n",
        "        #fold=0\n",
        "        for train_idx, val_idx in kf.split(full_dataset):\n",
        "            #print(f\" \\nFold {fold}\")\n",
        "            #fold+=1\n",
        "\n",
        "            # Split data\n",
        "            train_sub = Subset(full_dataset, train_idx)\n",
        "            val_sub = Subset(full_dataset, val_idx)\n",
        "\n",
        "            train_loader = DataLoader(train_sub, batch_size=256, shuffle=True)\n",
        "            val_loader = DataLoader(val_sub, batch_size=256, shuffle=True)\n",
        "\n",
        "            # Initialize Model, Loss, Optimizer\n",
        "            model = SimpleConvNet()\n",
        "            loss_fn = nn.CrossEntropyLoss() # Use Cross Entropy loss\n",
        "            #optimizer =Adam(model.parameters(), lr=lr, weight_decay=wd) # Adam optimizer\n",
        "            optimizer =SGD(model.parameters(), lr=lr, weight_decay=wd) # SGD optimizer\n",
        "\n",
        "            # Train for n_epochs\n",
        "            model.train()\n",
        "\n",
        "            for epoch in range(n_epochs):\n",
        "                running_loss = 0.0\n",
        "                for batch_X, batch_y in train_loader:\n",
        "                    optimizer.zero_grad()\n",
        "                    logits = model(batch_X)\n",
        "                    loss = loss_fn(logits, batch_y)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    running_loss += loss.item() / len(train_loader)\n",
        "            #print(f\"training loss = {running_loss : 0.4f}\")\n",
        "            train_loss.append(running_loss)\n",
        "            # Validate\n",
        "            model.eval()\n",
        "            correct_predictions=0\n",
        "            total_predictions = 0\n",
        "            val_loss=0.0\n",
        "            with torch.no_grad():\n",
        "                for batch_X, batch_y in val_loader:\n",
        "                    logits = model(batch_X)\n",
        "                    loss = loss_fn(logits, batch_y)\n",
        "                    val_loss += loss.item() / len(val_loader)\n",
        "                    predictions = torch.argmax(logits, dim=1)\n",
        "                    correct_predictions += (predictions == batch_y).sum().item()\n",
        "                    total_predictions += batch_y.size(0)\n",
        "                val_acc= correct_predictions/total_predictions\n",
        "            #print(f\"validation loss = {val_loss : 0.4f}\")\n",
        "            #print(f\"validation accuracy = {val_acc : 0.4f}\")\n",
        "\n",
        "            validation_loss.append(val_loss)\n",
        "            #fold_accuracies.append(correct_predictions / total_predictions)\n",
        "            fold_accuracies.append(val_acc)\n",
        "\n",
        "        mean_val_acc = np.mean(fold_accuracies) # Choose hyperparameters by mean validation accuracy\n",
        "        mean_train_loss=np.mean(train_loss)\n",
        "        mean_val_loss=np.mean(validation_loss)\n",
        "\n",
        "        print(\"-----------\")\n",
        "        print(f\"Learning Rate: {lr}, Weight Decay: {wd}\")\n",
        "        print(f\"Train_Loss ={mean_train_loss:0.4f}|| Validation_loss ={mean_val_loss:0.4f}|| Validation_accurancy ={mean_val_acc:.4f},\")\n",
        "\n",
        "        if mean_val_acc > best_acc:\n",
        "            best_acc = mean_val_acc\n",
        "            best_params = (lr, wd)\n",
        "print(\"\\n------------------------------\")\n",
        "print(f\"Best Parameters: Learning Rate={best_params[0]}, Weight Decay={best_params[1]} with Validation Accuracy={best_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBlrOXIBWIIW",
        "outputId": "7367763e-ef2e-43a6-d1dd-8d46cc105e93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyper parameter Grid\n",
            "Learning Rates: [0.0001, 0.001, 0.01]\n",
            "Weight Decay: [0, 0.0001, 0.001, 0.01]\n",
            "--------------------\n",
            "Starting Grid Search CV...\n",
            "-----------\n",
            "Learning Rate: 0.0001, Weight Decay: 0\n",
            "Train_Loss =0.6943|| Validation_loss =0.6939|| Validation_accurancy =0.4950,\n",
            "-----------\n",
            "Learning Rate: 0.0001, Weight Decay: 0.0001\n",
            "Train_Loss =0.6935|| Validation_loss =0.6944|| Validation_accurancy =0.5215,\n",
            "-----------\n",
            "Learning Rate: 0.0001, Weight Decay: 0.001\n",
            "Train_Loss =0.6939|| Validation_loss =0.6928|| Validation_accurancy =0.4860,\n",
            "-----------\n",
            "Learning Rate: 0.0001, Weight Decay: 0.01\n",
            "Train_Loss =0.6932|| Validation_loss =0.6927|| Validation_accurancy =0.5230,\n",
            "-----------\n",
            "Learning Rate: 0.001, Weight Decay: 0\n",
            "Train_Loss =0.6921|| Validation_loss =0.6920|| Validation_accurancy =0.5210,\n",
            "-----------\n",
            "Learning Rate: 0.001, Weight Decay: 0.0001\n",
            "Train_Loss =0.6934|| Validation_loss =0.6917|| Validation_accurancy =0.5190,\n",
            "-----------\n",
            "Learning Rate: 0.001, Weight Decay: 0.001\n",
            "Train_Loss =0.6921|| Validation_loss =0.6926|| Validation_accurancy =0.4965,\n",
            "-----------\n",
            "Learning Rate: 0.001, Weight Decay: 0.01\n",
            "Train_Loss =0.6915|| Validation_loss =0.6911|| Validation_accurancy =0.5705,\n",
            "-----------\n",
            "Learning Rate: 0.01, Weight Decay: 0\n",
            "Train_Loss =0.6820|| Validation_loss =0.6833|| Validation_accurancy =0.6425,\n",
            "-----------\n",
            "Learning Rate: 0.01, Weight Decay: 0.0001\n",
            "Train_Loss =0.6797|| Validation_loss =0.6808|| Validation_accurancy =0.6610,\n",
            "-----------\n",
            "Learning Rate: 0.01, Weight Decay: 0.001\n",
            "Train_Loss =0.6787|| Validation_loss =0.6796|| Validation_accurancy =0.6785,\n",
            "-----------\n",
            "Learning Rate: 0.01, Weight Decay: 0.01\n",
            "Train_Loss =0.6828|| Validation_loss =0.6829|| Validation_accurancy =0.7050,\n",
            "\n",
            "------------------------------\n",
            "Best Parameters: Learning Rate=0.01, Weight Decay=0.01 with Validation Accuracy=0.7050\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Un-DJI2nXb23",
        "outputId": "a3a02df3-bce5-408b-9071-d3f552b64cb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.01, 0.01)"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final training on all data :\n",
        "full_dataset = TensorDataset(X_raw, Y_raw)\n",
        "full_loader = DataLoader(full_dataset, batch_size=50, shuffle=True)\n",
        "\n",
        "# Initialize the final model with best hyperparameters:\n",
        "final_model = SimpleConvNet() # Create a new instance of the model\n",
        "loss_fn = nn.CrossEntropyLoss() # Define loss function\n",
        "#final_optimizer = Adam(final_model.parameters(), lr=best_params[0], weight_decay=best_params[1])\n",
        "final_optimizer = SGD(final_model.parameters(), lr=best_params[0], weight_decay=best_params[1])\n",
        "# Train the final model on the entire dataset\n",
        "#n_epochs = 50\n",
        "n_epochs = 200\n",
        "print(f\"Training final model with LR={best_params[0]} and WD={best_params[1]} for {n_epochs} epochs\")\n",
        "print(\"------------\")\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    final_model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for x_batch, y_batch in full_loader:\n",
        "        final_optimizer.zero_grad()\n",
        "        logits = final_model(x_batch)\n",
        "        loss = loss_fn(logits, y_batch)\n",
        "        loss.backward()\n",
        "        final_optimizer.step()\n",
        "        running_loss += loss.item() / len(full_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch} \\t Training Loss: {running_loss:.4f}\")\n",
        "\n",
        "print(\"Final model training complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_JtPq1qW8Au",
        "outputId": "5e7f4d54-0616-43ec-8ca8-dba4ee9e1fce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training final model with LR=0.01 and WD=0.01 for 100 epochs\n",
            "------------\n",
            "Epoch 0 \t Training Loss: 0.6927\n",
            "Epoch 1 \t Training Loss: 0.6907\n",
            "Epoch 2 \t Training Loss: 0.6897\n",
            "Epoch 3 \t Training Loss: 0.6890\n",
            "Epoch 4 \t Training Loss: 0.6877\n",
            "Epoch 5 \t Training Loss: 0.6867\n",
            "Epoch 6 \t Training Loss: 0.6859\n",
            "Epoch 7 \t Training Loss: 0.6850\n",
            "Epoch 8 \t Training Loss: 0.6842\n",
            "Epoch 9 \t Training Loss: 0.6823\n",
            "Epoch 10 \t Training Loss: 0.6800\n",
            "Epoch 11 \t Training Loss: 0.6790\n",
            "Epoch 12 \t Training Loss: 0.6764\n",
            "Epoch 13 \t Training Loss: 0.6733\n",
            "Epoch 14 \t Training Loss: 0.6703\n",
            "Epoch 15 \t Training Loss: 0.6667\n",
            "Epoch 16 \t Training Loss: 0.6611\n",
            "Epoch 17 \t Training Loss: 0.6546\n",
            "Epoch 18 \t Training Loss: 0.6497\n",
            "Epoch 19 \t Training Loss: 0.6404\n",
            "Epoch 20 \t Training Loss: 0.6301\n",
            "Epoch 21 \t Training Loss: 0.6144\n",
            "Epoch 22 \t Training Loss: 0.5902\n",
            "Epoch 23 \t Training Loss: 0.5684\n",
            "Epoch 24 \t Training Loss: 0.5340\n",
            "Epoch 25 \t Training Loss: 0.5034\n",
            "Epoch 26 \t Training Loss: 0.4624\n",
            "Epoch 27 \t Training Loss: 0.4294\n",
            "Epoch 28 \t Training Loss: 0.3968\n",
            "Epoch 29 \t Training Loss: 0.3618\n",
            "Epoch 30 \t Training Loss: 0.3484\n",
            "Epoch 31 \t Training Loss: 0.3378\n",
            "Epoch 32 \t Training Loss: 0.3323\n",
            "Epoch 33 \t Training Loss: 0.3128\n",
            "Epoch 34 \t Training Loss: 0.3236\n",
            "Epoch 35 \t Training Loss: 0.2931\n",
            "Epoch 36 \t Training Loss: 0.2819\n",
            "Epoch 37 \t Training Loss: 0.2864\n",
            "Epoch 38 \t Training Loss: 0.2789\n",
            "Epoch 39 \t Training Loss: 0.2696\n",
            "Epoch 40 \t Training Loss: 0.2562\n",
            "Epoch 41 \t Training Loss: 0.2567\n",
            "Epoch 42 \t Training Loss: 0.2727\n",
            "Epoch 43 \t Training Loss: 0.2455\n",
            "Epoch 44 \t Training Loss: 0.2425\n",
            "Epoch 45 \t Training Loss: 0.2422\n",
            "Epoch 46 \t Training Loss: 0.2246\n",
            "Epoch 47 \t Training Loss: 0.2269\n",
            "Epoch 48 \t Training Loss: 0.2164\n",
            "Epoch 49 \t Training Loss: 0.2357\n",
            "Epoch 50 \t Training Loss: 0.2176\n",
            "Epoch 51 \t Training Loss: 0.2082\n",
            "Epoch 52 \t Training Loss: 0.2069\n",
            "Epoch 53 \t Training Loss: 0.2025\n",
            "Epoch 54 \t Training Loss: 0.2158\n",
            "Epoch 55 \t Training Loss: 0.2124\n",
            "Epoch 56 \t Training Loss: 0.1971\n",
            "Epoch 57 \t Training Loss: 0.2042\n",
            "Epoch 58 \t Training Loss: 0.1872\n",
            "Epoch 59 \t Training Loss: 0.1819\n",
            "Epoch 60 \t Training Loss: 0.1880\n",
            "Epoch 61 \t Training Loss: 0.1788\n",
            "Epoch 62 \t Training Loss: 0.1794\n",
            "Epoch 63 \t Training Loss: 0.1869\n",
            "Epoch 64 \t Training Loss: 0.1784\n",
            "Epoch 65 \t Training Loss: 0.1883\n",
            "Epoch 66 \t Training Loss: 0.1765\n",
            "Epoch 67 \t Training Loss: 0.1843\n",
            "Epoch 68 \t Training Loss: 0.1862\n",
            "Epoch 69 \t Training Loss: 0.1776\n",
            "Epoch 70 \t Training Loss: 0.1735\n",
            "Epoch 71 \t Training Loss: 0.1627\n",
            "Epoch 72 \t Training Loss: 0.1653\n",
            "Epoch 73 \t Training Loss: 0.1553\n",
            "Epoch 74 \t Training Loss: 0.1789\n",
            "Epoch 75 \t Training Loss: 0.1647\n",
            "Epoch 76 \t Training Loss: 0.1681\n",
            "Epoch 77 \t Training Loss: 0.1617\n",
            "Epoch 78 \t Training Loss: 0.1543\n",
            "Epoch 79 \t Training Loss: 0.1612\n",
            "Epoch 80 \t Training Loss: 0.1546\n",
            "Epoch 81 \t Training Loss: 0.1586\n",
            "Epoch 82 \t Training Loss: 0.1458\n",
            "Epoch 83 \t Training Loss: 0.1476\n",
            "Epoch 84 \t Training Loss: 0.1473\n",
            "Epoch 85 \t Training Loss: 0.1439\n",
            "Epoch 86 \t Training Loss: 0.1505\n",
            "Epoch 87 \t Training Loss: 0.1532\n",
            "Epoch 88 \t Training Loss: 0.1438\n",
            "Epoch 89 \t Training Loss: 0.1415\n",
            "Epoch 90 \t Training Loss: 0.1376\n",
            "Epoch 91 \t Training Loss: 0.1534\n",
            "Epoch 92 \t Training Loss: 0.1418\n",
            "Epoch 93 \t Training Loss: 0.1355\n",
            "Epoch 94 \t Training Loss: 0.1376\n",
            "Epoch 95 \t Training Loss: 0.1509\n",
            "Epoch 96 \t Training Loss: 0.1433\n",
            "Epoch 97 \t Training Loss: 0.1402\n",
            "Epoch 98 \t Training Loss: 0.1335\n",
            "Epoch 99 \t Training Loss: 0.1426\n",
            "Final model training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load and predict on Test Data\n",
        "data_test = torch.load(\"/content/drive/MyDrive/Colab Notebooks/deepfake detector/hw2_test-1.pt\")\n",
        "X_test_raw= data_test[1]\n",
        "Y_test_ids = data_test[0]\n",
        "\n",
        "#X_test_emb = extract_features(X_test_raw)\n",
        "final_model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = final_model(X_test_raw)\n",
        "    predictions = torch.argmax(test_outputs, dim=1).numpy()\n",
        "\n",
        "# Save to CSV file\n",
        "df = pd.DataFrame({'id': Y_test_ids, 'label': predictions})\n",
        "df.to_csv(\"predictions.csv\", index=False)"
      ],
      "metadata": {
        "id": "7Y39Us5RXOl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion:\n",
        "\n",
        "1. The Best Model is:\n",
        "\n",
        "- Simple Convolutional Net with 3 blocks and one MLP classifier at the end\n",
        "  - With Hyperparameters\n",
        "    - Learning Rate=0.001,\n",
        "    - Weight Decay=0.0001\n",
        "  - Dropout probability = 0.5\n",
        "  - Adam optimizer\n",
        "  - Yeilds\n",
        "    - Validation Accuracy=0.9500\n",
        "    - Test set accuracy 0.944\n",
        "\n",
        "2. Why a Custom ConvNet Beats the Baseline:\n",
        "\n",
        "- Since we are freezing the ResNet in the\n",
        "\"extract feature\" function, i.e we don't backpropagate into the ResNet thus the overall model is not able to tune those existing convolutional filters to specifically detect deepfake artifacts. We are just using the ResNet for feature extraction purpose.\n",
        "\n",
        "- ResNet pretrained on ImageNet focuses more on semantic features like:\n",
        "ears,face,object shape. Not fake artifacts.\n",
        "\n",
        "- Deepfake detection depends heavily on:\n",
        "pixel noise, texture inconsistencies, local patterns.\n",
        "\n",
        "  By directly training a simple ConvNet from scratch on the raw images, my model can learn completely new, custom filters that are 100% focused on finding those unique fake patterns, which is exactly why the simple ConvNet might outperform the pretrained ResNet in this case.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NIdoyL4XOJ0L"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8oU9dPlzPtDq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}